{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Cleaning Notebook](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Cleaning Notebook](#toc1_)    \n",
    "- [**High Level Data Cleaning Process**](#toc2_)    \n",
    "  - [***Gather***](#toc2_1_)    \n",
    "  - [***Assess***](#toc2_2_)    \n",
    "  - [***Clean***](#toc2_3_)    \n",
    "    - [**Clean Data has Two Dimensions:**](#toc2_3_1_)    \n",
    "      - [**Principles of High Quality Data**:](#toc2_3_1_1_)    \n",
    "      - [**Principles of Tidy Data**:](#toc2_3_1_2_)    \n",
    "      - [**Ideal Order for Addressing Issues**:](#toc2_3_1_3_)    \n",
    "- [**Further Pre-Processing Concept Discussion**](#toc3_)    \n",
    "  - [Data Cleaning](#toc3_1_)    \n",
    "  - [Feature Scaling](#toc3_2_)    \n",
    "  - [Feature Encoding](#toc3_3_)    \n",
    "  - [Feature Transformation](#toc3_4_)    \n",
    "  - [Stratification](#toc3_5_)    \n",
    "  - [Dimensionality Reduction](#toc3_6_)    \n",
    "  - [Train-Test Split](#toc3_7_)    \n",
    "  - [Handling Imbalanced Data](#toc3_8_)    \n",
    "  - [Handling Time-Series Data](#toc3_9_)    \n",
    "  - [Handling Text Data](#toc3_10_)    \n",
    "  - [Handling Missing Data](#toc3_11_)    \n",
    "    - [High-level principles for imputing missing data points:](#toc3_11_1_)    \n",
    "    - [Strategies for Missing Data:](#toc3_11_2_)    \n",
    "  - [Handling Duplicate Data](#toc3_12_)    \n",
    "    - [High-level principles for handling duplicate data:](#toc3_12_1_)    \n",
    "    - [Strategies for Duplicate Data:](#toc3_12_2_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[**High Level Data Cleaning Process**](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <a id='toc2_1_'></a>[***Gather***](#toc0_)\n",
    "1. **Setup Libraries**: Import all the necessary libraries that you will need for your data analysis. This typically includes libraries like pandas, numpy, matplotlib, seaborn, etc. Setting up libraries at the beginning of your script ensures you have all the tools you need for analysis, visualization, and modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. **Load in Data**: Read in the data from your source file (like a CSV, Excel, SQL database, etc.) into a DataFrame, which is a type of data structure provided by the pandas library. This is your starting point for the data analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mdata.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_2_'></a>[***Assess***](#toc0_)\n",
    "1. **Programmatic Assessment**: Review the data using code. This includes methods like  `df.info()`, `df.describe()`, etc. These methods help you understand the structure of the data, the types of variables you have, and basic statistics of the variables.\n",
    "\n",
    "2. **Visual Assessment**: Review the data by scrolling through it in a spreadsheet or using `df.head()`, `df.tail()`. This can help you spot anomalies or patterns in the data that may not be immediately apparent through programmatic assessment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <a id='toc2_3_'></a>[***Clean***](#toc0_)\n",
    "1. **Define**: Define how you will clean the issue in words. This is your plan of action for dealing with the identified data quality and tidiness issues. It's important to define this plan before you start coding to ensure you have a clear understanding of the steps you need to take.\n",
    "\n",
    "2. **Code**: Convert your definitions into executable code. This is where you implement your plan. This could involve writing functions to clean the data, using built-in pandas functions, or using other data cleaning libraries.\n",
    "\n",
    "3. **Test**: Test your data to ensure your code was implemented correctly. This involves checking your cleaned data to confirm that it's in the expected format and that the data quality and tidiness issues have been addressed. This can be done using a combination of programmatic and visual assessments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_1_'></a>[**Clean Data has Two Dimensions:**](#toc0_)\n",
    "\n",
    "\n",
    "#### <a id='toc2_3_1_1_'></a>[**Principles of High Quality Data**:](#toc0_)\n",
    "1. **Completeness**: Do we have all of the records that we should?\n",
    "2. **Validity**: We have the records, but they're not valid, i.e., they don't conform to a defined schema, also known as a defined set of rules for data.\n",
    "3. **Accuracy**: Inaccurate data is wrong data that is valid. It adheres to the defined schema, but it is still incorrect.\n",
    "4. **Consistency**: Inconsistent data is both valid and accurate, but there are multiple correct ways of referring to the same thing.\n",
    "\n",
    "#### <a id='toc2_3_1_2_'></a>[**Principles of Tidy Data**:](#toc0_)\n",
    "- Tidy data is organized with three qualities in mind:\n",
    "    - **Columns:** Each variable forms a column.\n",
    "    - **Rows:** Each observation forms a row.\n",
    "    - **Tables:** Each type of observational unit forms a table.\n",
    "\n",
    "\n",
    "#### <a id='toc2_3_1_3_'></a>[**Ideal Order for Addressing Issues**:](#toc0_)\n",
    "\n",
    " 1. **Completeness issues** or **Fix Missing Data**: It's important to do this upfront so that subsequent data cleaning will not have to be repeated.\n",
    " 2. **Tidiness Issues**: Tidy datasets with data quality issues are almost always easier to clean than untidy datasets with the same issues.\n",
    " 3. **Quality Control**: Address the remaining validity, accuracy, and consistency issues in that order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[**Further Pre-Processing Concept Discussion**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_1_'></a>[Data Cleaning](#toc0_)\n",
    "\n",
    "- **Definition:** Data cleaning, also known as data cleansing or scrubbing, is the process of identifying and correcting or removing errors, inaccuracies, and inconsistencies in datasets. This could include dealing with missing or null values, duplicate data, irrelevant data, and outliers.\n",
    "\n",
    "- **Use Case and Intuition:** Data cleaning is crucial in any data analysis process as it ensures the quality and reliability of the data. The intuition behind it is that cleaner data leads to more accurate and reliable results from any subsequent data analysis or machine learning model.\n",
    "\n",
    "- **Example:** Removing duplicate rows in a dataset, replacing missing values with the mean or median of the rest of the data, or dropping irrelevant columns.\n",
    "\n",
    "- **High-Level Principles:** Data cleaning should be done carefully to avoid introducing bias into the data. It's also important to document the cleaning process for reproducibility and review.\n",
    "\n",
    "- **Assumptions and Cautions:** The process assumes that the data errors can be found and corrected without introducing significant bias. Care should be taken not to distort the data during cleaning, as it can lead to misleading results.\n",
    "\n",
    "- **Impact on ML Models:** Cleaner data can lead to more accurate and reliable machine learning models. On the other hand, poorly cleaned data can lead to inaccurate models and misleading results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <a id='toc3_2_'></a>[Feature Scaling](#toc0_)\n",
    "\n",
    "- **Definition:** Feature scaling is a method used to normalize the range of independent variables or features of data. Common methods include min-max normalization and standardization (z-score normalization).\n",
    "\n",
    "- **Use Case and Intuition:** Feature scaling is used when the features have different ranges. The intuition is that many machine learning algorithms perform better when numerical input variables are scaled to a standard range.\n",
    "\n",
    "- **Example:** Scaling age and income variables to the same range for a machine learning model predicting credit risk.\n",
    "\n",
    "- **High-Level Principles:** All features should be scaled in the same way. The same scaling parameters should be used for training and testing data.\n",
    "\n",
    "- **Assumptions and Cautions:** Assumes that the data is mostly normally distributed. Outliers can distort the result of scaling, so consider handling outliers before scaling.\n",
    "\n",
    "- **Impact on ML Models:** Feature scaling can speed up the training process and can lead to better performance for many machine learning algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <a id='toc3_3_'></a>[Feature Encoding](#toc0_)\n",
    "\n",
    "- **Definition:** Feature encoding is the process of converting categorical data into a form that could be provided to machine learning algorithms to improve their performance.\n",
    "\n",
    "- **Use Case and Intuition:** Used when dealing with categorical data. The intuition is that machine learning algorithms work better with numerical data, so categorical data is often encoded to numerical values.\n",
    "\n",
    "- **Example:** Encoding a feature like \"color\" with values \"red\", \"green\", \"blue\" to numerical values like 1, 2, 3.\n",
    "- \n",
    "- **5 Common Usages**:\n",
    "    1. One-Hot Encoding: It is a process of converting categorical data variables so they can be provided to machine learning algorithms to improve predictions. One hot encoding is a crucial part of feature engineering for machine learning.\n",
    "    2. Binning: The process of transforming continuous numerical variables into discrete categories for grouped analysis.\n",
    "    3. Polynomial Features: It is used to create interactions among features.\n",
    "    4. Custom Transformations: Logarithmic, square roots, or reciprocals to reduce the skewness of data.\n",
    "    5. Date/Time Features: Extracting information like 'month of the year', 'day of the week', 'hour of the day', etc.\n",
    "\n",
    "- **High-Level Principles:** Choose an appropriate encoding method based on the nature of the data. For example, ordinal encoding for ordinal data and one-hot encoding for nominal data.\n",
    "\n",
    "- **Assumptions and Cautions:** Assumes that the categorical variable can be adequately represented as numerical values. Be aware of the \"curse of dimensionality\" when using one-hot encoding.\n",
    "\n",
    "- **Impact on ML Models:** Proper feature encoding can lead to better performance of machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <a id='toc3_4_'></a>[Feature Transformation](#toc0_)\n",
    "\n",
    "- **Definition:** Feature transformation is the process of modifying existing features to better represent the underlying data patterns, or to meet the assumptions of the applied machine learning algorithms.\n",
    "\n",
    "- **Use Case and Intuition:** Used when the relationship between features and target variable is not linear, or when the data does not meet the assumptions of the machine learning algorithm. The intuition is that transformed features may expose better the data structure to the model.\n",
    "\n",
    "- **Example:** Applying log transformation to a feature to reduce skewness.\n",
    "\n",
    "- **5 Common Usages**:\n",
    "    1. Log Transformation: Used when data is highly skewed, it can help to reduce the skewness.\n",
    "    2. Square Root Transformation: This is a moderately strong transformation with a substantial effect on distribution shape.\n",
    "    3. Box-Cox Transformation: This is a family of power transformations indexed by a parameter lambda. When lambda is zero, the Box-Cox transformation equals the log transformation.\n",
    "    4. Yeo-Johnson Transformation: This is similar to the Box-Cox transformation but can be used on datasets containing zero and negative values.\n",
    "    5. Quantile Transformation: This transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values.\n",
    "\n",
    "\n",
    "- **High-Level Principles:** Choose an appropriate transformation based on the nature of the data and the requirements of the machine learning algorithm.\n",
    "\n",
    "- **Assumptions and Cautions:** Assumes that a transformation can better expose the data structure. Be aware that some transformations may make the data harder to interpret.\n",
    "\n",
    "- **Impact on ML Models:** Feature transformation can lead to better performance of machine learning models by meeting their assumptions or exposing better the data structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_5_'></a>[Stratification](#toc0_)\n",
    "\n",
    "- **Definition**: Stratification is the process of dividing members of the population into homogeneous subgroups before sampling. The strata should define a partition of the population. That is, it should be collectively exhaustive and mutually exclusive: every element in the population must be assigned to one and only one stratum.\n",
    "\n",
    "- **Use case and Intuition**: Stratification is used when an entity wants to ensure that the sample represents certain characteristics in the population. The strata are formed based on members' shared attributes or characteristics such as income level, education level, etc.\n",
    "\n",
    "- **5 Common Usages**:\n",
    "    1. Stratified Random Sampling: In statistical surveys, when populations are divided into strata, a random sample is taken from each stratum in a number that is proportional to the stratum's size when compared to the population. These subsets of the strata are then pooled to form a random sample.\n",
    "    2. Stratified Shuffle Split: It is a merge of Stratified K-Fold and Shuffle Split, which returns stratified randomized folds. The folds are made by preserving the percentage of samples for each class.\n",
    "    3. Stratified Cross-Validation: In stratified cross-validation, the folds are selected so that the mean response value is approximately equal in all the folds. In the case of a dichotomous classification, this means that each fold contains roughly the same proportions of the two types of class labels.\n",
    "    4. Stratified Train/Test Split: It is used in the splitting of data in a way that preserves the same proportions of examples in each class as observed in the original dataset.\n",
    "    5. Stratified Sampling for Handling Imbalanced Datasets: In imbalanced datasets, stratified sampling can help in ensuring that the train, validation, and test sets have the same proportion of samples for each class as found in the original dataset.\n",
    "\n",
    "- **Assumptions and Cautions**: Stratification assumes that the population is easily divisible into discrete subgroups. If stratification is done incorrectly, and the strata or layers do not accurately represent the population, then it can lead to selection bias, significantly reducing the statistical power of the output.\n",
    "\n",
    "- **Interpretation**: Stratification ensures that each subset of the dataset has the same proportions of the different target classes as the original dataset. This is particularly useful in classification problems where the target class is imbalanced.\n",
    "\n",
    "- **Assumptions and Cautions**: Feature engineering is more of an art than a science, and it heavily depends on the dataset and the problem at hand. It's always important to understand the underlying data and the business problem before deciding on the most appropriate feature engineering techniques.\n",
    "\n",
    "- **Interpretation**: Feature engineering can significantly improve the performance of machine learning models by creating meaningful features from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <a id='toc3_6_'></a>[Dimensionality Reduction](#toc0_)\n",
    "\n",
    "- **Definition:** Dimensionality reduction is the process of reducing the number of random variables under consideration, by obtaining a set of principal variables.\n",
    "\n",
    "- **Use Case and Intuition:** Used when dealing with high-dimensional data. The intuition is that reducing the dimensionality can help to remove noise and redundancy in the data, and can make the data easier to visualize and understand.\n",
    "\n",
    "- **Example:** Using Principal Component Analysis (PCA) to reduce the dimensionality of a dataset.\n",
    "\n",
    "- **High-Level Principles:** Choose an appropriate method based on the nature of the data. Be aware that dimensionality reduction can lead to loss of information.\n",
    "\n",
    "- **Assumptions and Cautions:** Assumes that the data has redundancy or noise that can be removed. Be aware that the reduced dimensions may be harder to interpret.\n",
    "\n",
    "- **Impact on ML Models:** Dimensionality reduction can lead to faster training times and better performance by removing noise and redundancy, but it can also lead to loss of information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <a id='toc3_7_'></a>[Train-Test Split](#toc0_)\n",
    "\n",
    "- **Definition:** Train-test split is a technique for evaluating the performance of a machine learning model. It involves splitting the dataset into two subsets: a training set used to train the model, and a test set used to evaluate the model.\n",
    "\n",
    "- **Use Case and Intuition:** Used in virtually all machine learning projects. The intuition is that evaluating the model on unseen data gives a better indication of the model's performance on new data.\n",
    "\n",
    "- **Example:** Splitting a dataset into 70% training data and 30% test data.\n",
    "\n",
    "- **High-Level Principles:** The split should be random and representative of the overall distribution of the data. The same split should be used for all models that are being compared.\n",
    "\n",
    "- **Assumptions and Cautions:** Assumes that the test set is representative of new data. Be aware of overfitting if the model performs well on the training data but poorly on the test data.\n",
    "\n",
    "- **Impact on ML Models:** Proper train-test split can give a better indication of the model's performance on new data, helping to choose the best model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <a id='toc3_8_'></a>[Handling Imbalanced Data](#toc0_)\n",
    "\n",
    "- **Definition:** Imbalanced data refers to a classification problem where the classes are not represented equally. Handling imbalanced data involves techniques to balance the classes, such as oversampling the minority class, undersampling the majority class, or using a combination of both.\n",
    "\n",
    "- **Use Case and Intuition:** Used when dealing with imbalanced classification problems. The intuition is that machine learning algorithms can be biased towards the majority class, leading to poor performance on the minority class.\n",
    "\n",
    "- **Example:** Using SMOTE (Synthetic Minority Over-sampling Technique) to oversample the minority class in a fraud detection problem.\n",
    "\n",
    "- **High-Level Principles:** Choose an appropriate method based on the nature of the data and the problem. Be aware that balancing the classes can lead to overfitting on the minority class.\n",
    "\n",
    "- **Assumptions and Cautions:** Assumes that the imbalance in the classes is causing poor performance on the minority class. Be aware of the trade-off between improving performance on the minority class and potentially worsening performance on the majority class.\n",
    "\n",
    "- **Impact on ML Models:** Handling imbalanced data can improve performance on the minority class, but it can also lead to overfitting on the minority class and potentially worse performance on the majority class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_9_'></a>[Handling Time-Series Data](#toc0_)\n",
    "\n",
    "- **Definition:** Time-series data is a sequence of data points indexed in time order. Handling time-series data involves techniques specific to this type of data, such as dealing with seasonality, trend, autocorrelation, and time-dependent variance (heteroscedasticity).\n",
    "\n",
    "- **Use Case and Intuition:** Used when dealing with data collected over time, such as stock prices or weather data. The intuition is that time-series data often has temporal dependencies that need to be accounted for in the analysis or modeling process.\n",
    "\n",
    "- **Example:** Using differencing to remove trend and seasonality in a time-series forecasting model.\n",
    "\n",
    "- **High-Level Principles:** Time-series data should be analyzed and modeled with techniques that account for its temporal dependencies. The data should also be checked for stationarity, as many time-series models assume this.\n",
    "\n",
    "- **Assumptions and Cautions:** Assumes that the temporal dependencies in the data can be modeled. Be aware that time-series models can be sensitive to the choice of time period and can be affected by missing values or changes in trend or seasonality.\n",
    "\n",
    "- **Impact on ML Models:** Proper handling of time-series data can lead to more accurate forecasts. On the other hand, ignoring the temporal dependencies can lead to poor model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <a id='toc3_10_'></a>[Handling Text Data](#toc0_)\n",
    "\n",
    "- **Definition:** Text data, or unstructured data, is data that is not organized in a pre-defined manner or does not have a pre-defined data model. Handling text data involves techniques such as tokenization, stemming, lemmatization, and vectorization.\n",
    "\n",
    "- **Use Case and Intuition:** Used when dealing with text data, such as customer reviews or tweets. The intuition is that text data can contain valuable information, but it needs to be transformed into a numerical format that can be used by machine learning algorithms.\n",
    "\n",
    "- **Example:** Using TF-IDF (Term Frequency-Inverse Document Frequency) to vectorize customer reviews for sentiment analysis.\n",
    "\n",
    "- **High-Level Principles:** Text data should be preprocessed to remove noise (like punctuation and common words), and transformed into a numerical format. The choice of transformation can depend on the problem and the nature of the text.\n",
    "\n",
    "- **Assumptions and Cautions:** Assumes that the text data contains relevant information that can be extracted and used. Be aware that text data can be noisy and can require significant preprocessing.\n",
    "\n",
    "- **Impact on ML Models:** Proper handling of text data can lead to more accurate models when dealing with text data. On the other hand, poor handling of text data can lead to noisy and uninformative features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <a id='toc3_11_'></a>[Handling Missing Data](#toc0_)\n",
    "\n",
    "- **Definition:** Missing data occurs when no data value is stored for a variable in an observation. Handling missing data involves techniques such as imputation or deletion.\n",
    "\n",
    "- **Use Case and Intuition:** Used when dealing with datasets with missing values. The intuition is that missing data can lead to biased or incorrect results, so it's important to handle it appropriately.\n",
    "\n",
    "- **Example:** Using mean imputation to fill in missing values in a dataset.\n",
    "\n",
    "- **High-Level Principles:** The method for handling missing data should be chosen based on the nature of the data and the reason for the missingness. It's also important to consider the impact of the chosen method on the subsequent analysis or modeling.\n",
    "\n",
    "- **Assumptions and Cautions:** Assumes that the missing data can be accurately imputed or that it's safe to delete the missing values. Be aware that inappropriate handling of missing data can lead to biased or incorrect results.\n",
    "\n",
    "- **Impact on ML Models:** Proper handling of missing data can lead to more accurate and reliable models. On the other hand, poor handling of missing data can lead to biased or incorrect models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <a id='toc3_11_1_'></a>[High-level principles for imputing missing data points:](#toc0_)\n",
    "\n",
    "- Understand the mechanism of missingness: Data can be missing completely at random (MCAR), missing at random (MAR), or missing not at random (MNAR). The appropriate imputation method depends on which of these mechanisms is at work.\n",
    "\n",
    "- Preserve the relationships in the data: The imputation method should preserve the relationships between variables as much as possible.\n",
    "\n",
    "- Account for the uncertainty: The imputation method should account for the uncertainty of the imputed values. This is especially important for methods like multiple imputation.\n",
    "\n",
    "- Check the results: After imputation, check the results to ensure that the imputed data makes sense and that the statistical properties of the data have not been unduly distorted.\n",
    "\n",
    "- Document the process: Keep a record of what imputation methods were used, and why. This is important for the reproducibility of the analysis.\n",
    "\n",
    "### <a id='toc3_11_2_'></a>[Strategies for Missing Data:](#toc0_)\n",
    "\n",
    "Handling missing data is a critical step in the data preprocessing pipeline. Here are five common methods for dealing with missing data, along with best practices for each:\n",
    "\n",
    "1. **Listwise Deletion (Complete Case Analysis):** This method involves removing all data for an observation that has one or more missing values. \n",
    "\n",
    "   - **Best Practice:** Use this method when the data is missing completely at random, and the proportion of missing data is small. Be aware that this method can lead to a loss of information and reduced statistical power.\n",
    "\n",
    "2. **Pairwise Deletion:** This method involves deleting cases where the specific variable is missing that is currently being analyzed.\n",
    "\n",
    "   - **Best Practice:** Use this method when the data is missing completely at random. Be aware that this method can lead to different results for different analyses, depending on which cases are deleted.\n",
    "\n",
    "3. **Mean/Median/Mode Imputation:** This method involves replacing the missing values for a particular variable with the mean, median, or mode of the available cases.\n",
    "\n",
    "   - **Best Practice:** Use this method when the data is missing completely at random, and the variable is numerical (for mean or median imputation) or categorical (for mode imputation). Be aware that this method can lead to an underestimate of the variance and potentially biased estimates of the correlations between variables.\n",
    "\n",
    "4. **Last Observation Carried Forward (LOCF) or Next Observation Carried Backward (NOCB):** This method involves replacing the missing value with the last observed value (LOCF) or the next observed value (NOCB).\n",
    "\n",
    "   - **Best Practice:** Use this method for time-series data where the observations have a logical order. Be aware that this method can lead to biased estimates if the data is not missing at random.\n",
    "\n",
    "5. **Multiple Imputation:** This method involves creating multiple imputed datasets, analyzing each one separately, and then pooling the results.\n",
    "\n",
    "   - **Best Practice:** Use this method when the data is not missing at random, and the missingness can be modeled from the observed data. Be aware that this method is more complex and computationally intensive than the others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_12_'></a>[Handling Duplicate Data](#toc0_)\n",
    "\n",
    "- **Definition:** Duplicate data refers to repetitions of the same data entry/record in the dataset. Handling duplicate data involves identifying and dealing with these repetitions to ensure the quality and reliability of the dataset.\n",
    "\n",
    "- **Use Case and Intuition:** Duplicate data is often encountered in real-world datasets due to various reasons such as data entry errors, merging of datasets, etc. The intuition behind handling duplicate data is that duplicates can skew the data distribution and lead to biased analysis or machine learning models.\n",
    "\n",
    "- **Example:** In a customer database, the same customer might be recorded multiple times due to data entry errors. These duplicates can be identified by matching across several fields like name, address, and contact information, and then removed to avoid over-representing this customer in subsequent analyses.\n",
    "\n",
    "- **High-Level Principles:** The method for handling duplicates should be chosen based on the nature of the data and the reason for duplication. It's also important to consider the impact of the chosen method on the subsequent analysis or modeling.\n",
    "\n",
    "- **Assumptions and Cautions:** The process assumes that duplicates do not carry unique information. Care should be taken to ensure that the records are true duplicates across all features and not just a subset. Also, in some cases, duplicates might be meaningful and should not be removed (e.g., in transactional data, the same customer can make the same purchase multiple times).\n",
    "\n",
    "- **Impact on ML Models:** Proper handling of duplicate data can lead to more accurate and reliable models by ensuring a fair representation of all data points. On the other hand, not handling duplicate data can lead to overfitting towards the over-represented data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_12_1_'></a>[High-level principles for handling duplicate data:](#toc0_)\n",
    "\n",
    "- Understand the Cause: Before handling duplicate data, it's important to understand why the duplicates occurred. This can help in choosing the best method for handling them.\n",
    "\n",
    "- Preserve Information: The method for handling duplicates should preserve as much information as possible, unless the duplicates are likely due to errors.\n",
    "\n",
    "- Check the Results: After handling duplicates, check the results to ensure that the process has not introduced any errors or biases.\n",
    "\n",
    "- Document the Process: Keep a record of what methods were used to handle duplicates, and why. This is important for the reproducibility of the analysis.\n",
    "\n",
    "### <a id='toc3_12_2_'></a>[Strategies for Duplicate Data:](#toc0_)\n",
    "Handling duplicate data is another critical step in the data preprocessing pipeline. Here are five common methods for dealing with duplicate data, along with best practices for each:\n",
    "\n",
    "1. **Removal of Duplicates:** This method involves identifying and removing duplicate records in the dataset.\n",
    "\n",
    "   - **Best Practice:** Use this method when the duplicates do not provide any additional information and are likely to have occurred due to data entry errors. Be careful to ensure that the records are true duplicates across all features and not just a subset.\n",
    "\n",
    "2. **Averaging:** If the duplicates have slight variations in a continuous feature, you may choose to average the feature values across the duplicate records and keep a single record.\n",
    "\n",
    "   - **Best Practice:** Use this method when the duplicates are not exact duplicates and the variations in the continuous features are minor and likely due to measurement errors.\n",
    "\n",
    "3. **Majority Voting:** If the duplicates have variations in a categorical feature, you may choose to apply a majority voting scheme and keep the mode of the feature values across the duplicate records.\n",
    "\n",
    "   - **Best Practice:** Use this method when the duplicates are not exact duplicates and the variations in the categorical features are minor and likely due to data entry errors.\n",
    "\n",
    "4. **Keeping the Most Recent:** In time-series data, if duplicates are found, you may choose to keep the most recent record and discard the older ones.\n",
    "\n",
    "   - **Best Practice:** Use this method when the data is time-series and the duplicates are likely due to data being updated over time.\n",
    "\n",
    "5. **Combining Information:** If the duplicates have different information in different features, you may choose to combine the information into a single record.\n",
    "\n",
    "   - **Best Practice:** Use this method when the duplicates are not exact duplicates and the different information in the duplicates is valuable and should be preserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
