{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Neural Networks](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Neural Networks](#toc1_)    \n",
    "    - [Types of Layers in Neural Networks](#toc1_1_1_)    \n",
    "    - [Activation Functions](#toc1_1_2_)    \n",
    "    - [Optimization Algorithm](#toc1_1_3_)    \n",
    "    - [Optimization Algorithm Parameters](#toc1_1_4_)    \n",
    "    - [Batch Size](#toc1_1_5_)    \n",
    "    - [Epochs](#toc1_1_6_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_1_'></a>[Types of Layers in Neural Networks](#toc0_)\n",
    "\n",
    "- **Definition**: In a neural network, layers are interconnected nodes that are organized into columns. Each layer takes in input from previous layers (or the input data), performs transformations on this data, and passes its output to subsequent layers.\n",
    "\n",
    "- **Intuition**: Think of layers as filters of information. Each layer extracts some information from the input data, which is then passed on to the next layer for further processing.\n",
    "\n",
    "- **Purpose**: The purpose of having different types of layers is to allow the neural network to learn different types of features from the data. For example, convolutional layers are good at learning spatial features in image data, while recurrent layers are good at learning temporal features in time-series data.\n",
    "\n",
    "- **Formula**: There isn't a specific formula for layers in a neural network, as they are more of a structural concept. However, the transformations performed by a layer can often be represented mathematically. For example, a fully connected layer performs a matrix multiplication and adds a bias term.\n",
    "\n",
    "- **Code**: Here is an example of how to define a simple neural network with different types of layers in Python using the Keras library:\n",
    "\n",
    "```python\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "```\n",
    "\n",
    "- **Limitations and Cautions**: The choice of layer type can greatly affect the performance of a neural network. It's important to choose the right type of layer for the specific task at hand. For example, using a convolutional layer for image data is generally a good idea, but using it for time-series data might not be as effective.\n",
    "\n",
    "- **Subconcepts**: Some of the common types of layers in a neural network include:\n",
    "  - **Dense (or Fully Connected) Layers**: Every neuron in a dense layer is connected to every neuron in the previous layer.\n",
    "  - **Convolutional Layers**: These layers apply a convolution operation to the input, passing the result to the next layer. This is especially effective for tasks like image recognition.\n",
    "  - **Pooling Layers**: These layers reduce the spatial size of the convolved feature, reducing the computational complexity of the model.\n",
    "  - **Recurrent Layers**: These layers save the output of a layer and feed it back to the input in order to predict the output of the layer at the current time step given the previous time step.\n",
    "  - **Normalization Layers**: These layers standardize the inputs to the layer, helping to stabilize the learning process and reduce the number of training epochs required.\n",
    "  - **Dropout Layers**: These layers randomly set a fraction of input units to 0 at each update during training time, which helps prevent overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_2_'></a>[Activation Functions](#toc0_)\n",
    "\n",
    "- **Definition**: Activation functions are mathematical equations that determine the output of a neural network. The function is attached to each neuron in the network, and determines whether it should be activated (“fired”) or not, based on whether each neuron’s input is relevant for the model’s prediction.\n",
    "\n",
    "- **Intuition**: Activation functions are like the gatekeepers of the neural network. They decide how much information should proceed further through the network.\n",
    "\n",
    "- **Purpose**: They are used to introduce non-linearity into the output of a neuron. This is important because most real world data is non linear and we want neurons to learn these non linear representations.\n",
    "\n",
    "- **Formula**: There are many types of activation functions, each with its own formula. Here are a few examples:\n",
    "  - Sigmoid: $f(x) = 1 / (1 + e^{-x})$\n",
    "  - ReLU (Rectified Linear Unit): $f(x) = max(0, x)$\n",
    "  - Tanh: $f(x) = (e^{x} - e^{-x}) / (e^{x} + e^{-x})$\n",
    "\n",
    "- **Code**: Here is an example of how to use activation functions in Python using the Keras library:\n",
    "\n",
    "```python\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=50))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "```\n",
    "\n",
    "- **Limitations and Cautions**: The choice of activation function can greatly affect the performance of a neural network. It's important to choose the right activation function for the specific task at hand. For example, the ReLU activation function is often a good choice for hidden layers, but it wouldn't be a good choice for the output layer of a binary classification problem, where a sigmoid activation function would be more appropriate.\n",
    "\n",
    "- **Subconcepts**: Some of the common types of activation functions include:\n",
    "  - **Sigmoid**: This function maps the input values to a range between 0 and 1, making it useful for output neurons in binary classification.\n",
    "  - **ReLU (Rectified Linear Unit)**: This function sets all negative values in the input to 0 and leaves all positive values unchanged.\n",
    "  - **Tanh**: This function maps the input values to a range between -1 and 1.\n",
    "  - **Softmax**: This function is often used in the output layer of a multi-class classification neural network. It converts the outputs into probability values for each class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sigmoid Function\n",
    "- **Formula**: $f(x) = \\frac{1}{1 + e^{-x}}$\n",
    "- **Intuition**: The sigmoid function maps any input into a range between 0 and 1, making it useful for outputting probabilities.\n",
    "- **Use Case**: It is often used in the output layer of a binary classification problem where the output is expected to be a probability that gives the likelihood of the input belonging to a particular class.\n",
    "- **Limitations**: The sigmoid function suffers from the vanishing gradient problem, where the gradients become very small if the input is large. This can slow down learning during backpropagation. It also isn't zero-centered which can lead to undesirable zig-zagging dynamics in the gradient updates for the weights.\n",
    "- **Assumptions**: No specific assumptions are necessary for using the sigmoid function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Tanh Function\n",
    "- **Formula**: $f(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$\n",
    "- **Intuition**: The tanh function is similar to the sigmoid function but maps any input to a range between -1 and 1. This means that the output is zero-centered.\n",
    "- **Use Case**: It is often used in hidden layers of a neural network as it can model both positive and negative input values.\n",
    "- **Limitations**: Like the sigmoid function, the tanh function also suffers from the vanishing gradient problem.\n",
    "- **Assumptions**: No specific assumptions are necessary for using the tanh function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3. ReLU (Rectified Linear Unit) Function\n",
    "- **Formula**: $f(x) = \\max(0, x)$\n",
    "- **Intuition**: The ReLU function outputs the input directly if it is positive, otherwise, it outputs zero. It introduces non-linearity in the network without affecting the receptive fields of convolution layers.\n",
    "- **Use Case**: It is widely used in the hidden layers of neural networks as it helps the model learn complex patterns and overcome the vanishing gradient problem.\n",
    "- **Limitations**: The ReLU function suffers from the \"dying ReLU\" problem, where neurons can sometimes be stuck in the negative state and always output zero, causing them to stop learning.\n",
    "- **Assumptions**: No specific assumptions are necessary for using the ReLU function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4. Leaky ReLU Function\n",
    "- **Formula**: $f(x) = \\max(0.01x, x)$\n",
    "- **Intuition**: Leaky ReLU is a variant of ReLU that has a small slope for negative values instead of a flat zero, which helps to alleviate the dying ReLU problem.\n",
    "- **Use Case**: It can be used in the hidden layers of neural networks, especially when the dying ReLU problem is a concern.\n",
    "- **Limitations**: The value of the slope for negative inputs is a hyperparameter and needs to be manually tuned.\n",
    "- **Assumptions**: No specific assumptions are necessary for using the Leaky ReLU function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5. Softmax Function\n",
    "- **Formula**: $f(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{K} e^{x_j}}$ for i = 1, …, K and K is the number of classes.\n",
    "- **Intuition**: The softmax function outputs a vector that represents the probability distribution of a list of potential outcomes. It's the multiclass generalization of the sigmoid function.\n",
    "- **Use Case**: It is often used in the output layer of a neural network for multiclass classification problems.\n",
    "- **Limitations**: It can suffer from numerical instability due to the exponentials involved in its calculation.\n",
    "- **Assumptions**: No specific assumptions are necessary for using the softmax function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_3_'></a>[Optimization Algorithm](#toc0_)\n",
    "\n",
    "- **Definition**: Optimization algorithms in neural networks are used to minimize the error (loss function output) and improve the model's performance. They adjust the weights and biases of the model in order to minimize the output of the loss function.\n",
    "\n",
    "- **Intuition**: Think of the optimization process as a hiker (the optimization algorithm) trying to find the bottom of a valley (the minimum of the loss function) while only being able to see a few feet ahead (the current batch of data).\n",
    "\n",
    "- **Purpose**: The purpose of an optimization algorithm is to find the best set of weights and biases for the model that minimize the output of the loss function.\n",
    "\n",
    "- **Formula**: There isn't a specific formula for optimization algorithms as a whole, as each algorithm has its own method of updating the weights and biases. For example, the update rule for Stochastic Gradient Descent (SGD) is: $w = w - \\eta \\nabla L$, where $w$ is the weight, $\\eta$ is the learning rate, and $\\nabla L$ is the gradient of the loss function.\n",
    "\n",
    "- **Code**: Here is an example of how to use an optimization algorithm in Python using the Keras library:\n",
    "\n",
    "```python\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=50))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd)\n",
    "```\n",
    "\n",
    "- **Limitations and Cautions**: The choice of optimization algorithm can greatly affect the performance of a neural network. It's important to choose the right algorithm for the specific task at hand. For example, while SGD is a good general-purpose optimizer, it might struggle with problems where the loss function has many shallow minima.\n",
    "\n",
    "- **Subconcepts**: Some of the common types of optimization algorithms include:\n",
    "  - **Stochastic Gradient Descent (SGD)**: This is the most basic optimization algorithm. It updates the weights using the gradient of the loss function with respect to the weight.\n",
    "  - **Momentum**: This is a variant of SGD that takes into account the previous gradients to smooth out the update process.\n",
    "  - **Adagrad**: This algorithm adapts the learning rate to the parameters, performing smaller updates for parameters associated with frequently occurring features, and larger updates for parameters associated with infrequent features.\n",
    "  - **RMSprop**: This is an unpublished, adaptive learning rate method proposed by Geoff Hinton in his Coursera course.\n",
    "  - **Adam**: This algorithm combines the benefits of RMSprop and momentum by using moving averages of the parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <a id='toc1_1_4_'></a>[Optimization Algorithm Parameters](#toc0_)\n",
    "\n",
    "- **Definition**: These are the parameters that define how the optimization algorithm works. For example, the learning rate is a common parameter that determines how much the weights are updated during training.\n",
    "\n",
    "- **Intuition**: Think of these parameters as the settings on a machine. By adjusting these settings, you can change how the machine operates.\n",
    "\n",
    "- **Purpose**: The purpose of these parameters is to control the behavior of the optimization algorithm.\n",
    "\n",
    "- **Formula**: There isn't a specific formula for these parameters, as they are values that are set before the training process begins.\n",
    "\n",
    "- **Code**: Here is an example of how to set the parameters of an optimization algorithm in Python using the Keras library:\n",
    "\n",
    "```python\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=50))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd)\n",
    "```\n",
    "\n",
    "- **Limitations and Cautions**: The choice of parameters can greatly affect the performance of the optimization algorithm. It's important to choose the right values for your specific task. For example, a learning rate that is too high can cause the algorithm to overshoot the minimum of the loss function, while a learning rate that is too low can cause the training process to be very slow.\n",
    "\n",
    "- **Subconcepts**: Some of the common types of optimization algorithm parameters include:\n",
    "  - **Learning Rate**: This is the size of the steps the algorithm takes towards the minimum of the loss function.\n",
    "  - **Momentum**: This is a value between 0 and 1 that increases the size of the steps taken towards the minimum of the loss function.\n",
    "  - **Decay**: This is a value that reduces the learning rate over time, helping the algorithm to settle at the minimum of the loss function.\n",
    "  - **Nesterov Momentum**: This is a variant of momentum that has slightly better performance in practice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_5_'></a>[Batch Size](#toc0_)\n",
    "\n",
    "- **Definition**: Batch size is the number of training examples used in one iteration. For instance, let's say you have 1000 training samples and you decide to set batch size to 100. The algorithm takes the first 100 samples (from 1st to 100th) from the training dataset and trains the network. Next it takes the second 100 samples (from 101st to 200th) and trains the network again. This process continues until we have propagated through all samples of the network.\n",
    "\n",
    "- **Intuition**: Larger batch sizes result in faster progress in training, but don't always converge as fast. Smaller batch sizes train slower, but can converge faster. It's definitely problem dependent.\n",
    "\n",
    "- **Purpose**: The purpose of batch size is to allow the model to be trained using less memory space. By adjusting the batch size, you can ensure that your model is able to train on your machine's memory.\n",
    "\n",
    "- **Formula**: There isn't a specific formula for batch size, as it is a hyperparameter that you set before training the model.\n",
    "\n",
    "- **Code**: Here is an example of how to set the batch size in Python using the Keras library:\n",
    "\n",
    "```python\n",
    "model.fit(X_train, Y_train, epochs=10, batch_size=32)\n",
    "```\n",
    "\n",
    "- **Limitations and Cautions**: The choice of batch size can significantly affect the performance of your model. A batch size that is too large can lead to poor generalization (the model learns the training data too well and performs poorly on unseen data). On the other hand, a batch size that is too small can lead to slow convergence and a noisy gradient signal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <a id='toc1_1_6_'></a>[Epochs](#toc0_)\n",
    "\n",
    "- **Definition**: An epoch is a term used in machine learning and indicates the number of passes of the entire training dataset the machine learning algorithm has completed. If the batch size is the whole dataset then the number of epochs is the number of iterations.\n",
    "\n",
    "- **Intuition**: More epochs means the learning algorithm has more opportunities to tune the weights of the network to better map inputs to outputs. But more training isn't always better. A point of diminishing returns can be reached.\n",
    "\n",
    "- **Purpose**: The purpose of setting the number of epochs is to specify how long we want to train our neural network.\n",
    "\n",
    "- **Formula**: There isn't a specific formula for epochs, as it is a hyperparameter that you set before training the model.\n",
    "\n",
    "- **Code**: Here is an example of how to set the number of epochs in Python using the Keras library:\n",
    "\n",
    "```python\n",
    "model.fit(X_train, Y_train, epochs=10, batch_size=32)\n",
    "```\n",
    "\n",
    "- **Limitations and Cautions**: The choice of the number of epochs is critical. Too few epochs can mean underfitting of the model, whereas too many epochs can mean overfitting of the model. It's important to choose a suitable number of epochs so that the model can learn the data well without overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Hidden Layers\n",
    "\n",
    "**Definition**: The number of hidden layers in a neural network is a hyperparameter that determines the depth of the network. Each hidden layer is composed of a set of neurons, where each neuron is a computational unit that takes in input from the previous layer, applies a transformation, and passes the output to the next layer.\n",
    "\n",
    "**Intuition**: More hidden layers allow the network to learn more complex representations of the data. However, too many layers can lead to overfitting, where the model learns the training data too well and performs poorly on unseen data.\n",
    "\n",
    "**Use Case**: Deep learning models, such as convolutional neural networks (CNNs) for image recognition or recurrent neural networks (RNNs) for sequence data, often have multiple hidden layers.\n",
    "\n",
    "**Formula**: There is no specific formula for determining the optimal number of hidden layers. It is usually determined through experimentation and cross-validation.\n",
    "\n",
    "**Limitations**: Adding more hidden layers increases the computational complexity of the model and the risk of overfitting. It also makes the model more difficult to train effectively, as gradients can vanish or explode in deep networks (a problem known as the vanishing/exploding gradients problem).\n",
    "\n",
    "**Cautions**: It's important to balance the complexity of the model (number of hidden layers) with the amount and diversity of available training data. Regularization techniques, such as dropout or weight decay, can be used to prevent overfitting in deep networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Number of Neurons in Hidden Layers\n",
    "\n",
    "**Definition**: The number of neurons in a hidden layer is a hyperparameter that determines the width of the network. Each neuron in a layer takes in input from all neurons in the previous layer, applies a transformation, and passes the output to all neurons in the next layer.\n",
    "\n",
    "**Intuition**: More neurons allow the layer to learn more complex representations of the data. However, too many neurons can lead to overfitting.\n",
    "\n",
    "**Use Case**: The number of neurons in hidden layers is a key factor in the design of any neural network and is typically determined through experimentation and cross-validation.\n",
    "\n",
    "**Formula**: There is no specific formula for determining the optimal number of neurons. It is usually determined through experimentation and cross-validation.\n",
    "\n",
    "**Limitations**: Adding more neurons increases the computational complexity of the model and the risk of overfitting. It also increases the number of parameters in the model, making it more difficult to train effectively.\n",
    "\n",
    "**Cautions**: It's important to balance the complexity of the model (number of neurons) with the amount and diversity of available training data. Regularization techniques, such as dropout or weight decay, can be used to prevent overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Learning Rate\n",
    "\n",
    "**Definition**: The learning rate is a hyperparameter that determines the step size at each iteration while moving toward a minimum of a loss function. It controls how much we are adjusting the weights of our network with respect to the loss gradient.\n",
    "\n",
    "**Intuition**: A smaller learning rate could get stuck in local minima, while a larger learning rate could overshoot the global minimum.\n",
    "\n",
    "**Use Case**: The learning rate is a key factor in the training of any neural network and is typically determined through experimentation and cross-validation.\n",
    "\n",
    "**Formula**: The learning rate is typically a constant, but it can also be adjusted dynamically during training (a technique known as learning rate scheduling).\n",
    "\n",
    "**Limitations**: If the learning rate is too high, the model might converge too quickly to a suboptimal solution, or it might not converge at all. If the learning rate is too low, the model might take too long to converge, or it might get stuck in a local minimum.\n",
    "\n",
    "**Cautions**: It's important to choose an appropriate learning rate for the specific problem and model. Techniques such as learning rate scheduling or adaptive learning rates can be used to adjust the learning rate during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Activation Function\n",
    "\n",
    "**Definition**: The activation function is a mathematical function applied at each node in a layer, which determines the output of that node given an input or set of inputs.\n",
    "\n",
    "**Intuition**: Different activation functions can model different types of relationships between input and output, and some may work better than others for a particular task.\n",
    "\n",
    "**Use Case**: The choice of activation function can have a significant impact on the performance of a neural network. Common choices include ReLU, sigmoid, and tanh.\n",
    "\n",
    "**Formula**: The formula for the activation function depends on the specific function used. For example, the ReLU function is defined as f(x) = max(0, x), and the sigmoid function is defined as f(x) = 1 / (1 + exp(-x)).\n",
    "\n",
    "**Limitations**: The choice of activation function can affect the ability of the network to converge and the speed of convergence. Some activation functions, like the sigmoid function, can suffer from the vanishing gradient problem, which can slow down training.\n",
    "\n",
    "**Cautions**: It's important to choose an appropriate activation function for the specific problem and model. Different activation functions have different properties and are suitable for different types of tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras\n",
    "\n",
    "Originally, TensorFlow was a complex, low-level library for building neural networks. To overcome this complexity, a group of people created a separate library, called Keras, which was an interface to make it easier to build sophisticated neural networks in TensorFlow and other neural network libraries. Keras was so popular and widely used that as of TensorFlow 2.X, Keras is integrated directly into TensorFlow and is the primary interface used to build neural networks in TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Neural Network Using Keras\n",
    "\n",
    "The process of building a network using Keras can generally be broken down into four separate steps:\n",
    "1. **Build the model**: This is the step where we will declare the structure of the network — primarily the types and sizes of the hidden layers.\n",
    "2. **Compile the model**: This step allows us to customize some of the settings that will be used for training.\n",
    "3. **Train the model**\n",
    "4. **Evaluate the model and generate predictions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: 5 Hidden Layers with 5 Nodes Each\n",
    "\n",
    "Let's take a closer look at how to build a neural network using TensorFlow by recreating the network we create previously using `scikit-learn`'s MLP Classifier.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1WfEXuqomB66DpKcMvBR57nXTI9XXFV9d\" width=600 style=\"margin:20px 20px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, let's import the required modules we will need to create this network using TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same synthetic data from before to train our network. However, let's also generate a test set to allow us to further evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = generate_data()\n",
    "X_test, y_test = generate_data(random_seed = 1) # generate test data with a different random seed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1.** Build the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seeds for reproducibility\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "# Create a new sequential model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Declare the hidden layers\n",
    "model.add(layers.Dense(5, activation=\"relu\"))\n",
    "model.add(layers.Dense(5, activation=\"relu\"))\n",
    "model.add(layers.Dense(5, activation=\"relu\"))\n",
    "model.add(layers.Dense(5, activation=\"relu\"))\n",
    "model.add(layers.Dense(5, activation=\"relu\"))\n",
    "\n",
    "# Declare the output layer\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2.** Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    # Optimizer\n",
    "    optimizer=keras.optimizers.Adam(),  \n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    # Metric used to evaluate model\n",
    "    metrics=[keras.metrics.BinaryAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3.** Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7/7 [==============================] - 0s 929us/step - loss: 0.6838 - binary_accuracy: 0.5000\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 0s 572us/step - loss: 0.6777 - binary_accuracy: 0.5000\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 0s 715us/step - loss: 0.6724 - binary_accuracy: 0.5000\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 0s 643us/step - loss: 0.6679 - binary_accuracy: 0.5150\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 0s 720us/step - loss: 0.6627 - binary_accuracy: 0.5300\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 0s 790us/step - loss: 0.6571 - binary_accuracy: 0.5900\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 0s 572us/step - loss: 0.6506 - binary_accuracy: 0.6150\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 0s 608us/step - loss: 0.6427 - binary_accuracy: 0.6300\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 0s 501us/step - loss: 0.6338 - binary_accuracy: 0.6500\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 0s 571us/step - loss: 0.6241 - binary_accuracy: 0.6650\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 0s 879us/step - loss: 0.6127 - binary_accuracy: 0.6850\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 0s 858us/step - loss: 0.6004 - binary_accuracy: 0.7050\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 0s 731us/step - loss: 0.5869 - binary_accuracy: 0.7100\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 0s 647us/step - loss: 0.5739 - binary_accuracy: 0.7250\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 0s 429us/step - loss: 0.5595 - binary_accuracy: 0.7450\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 0s 925us/step - loss: 0.5462 - binary_accuracy: 0.7450\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 0s 676us/step - loss: 0.5324 - binary_accuracy: 0.7650\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 0s 572us/step - loss: 0.5199 - binary_accuracy: 0.7750\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 0s 573us/step - loss: 0.5084 - binary_accuracy: 0.7800\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 0s 695us/step - loss: 0.4977 - binary_accuracy: 0.7850\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 0s 571us/step - loss: 0.4873 - binary_accuracy: 0.7900\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 0s 572us/step - loss: 0.4781 - binary_accuracy: 0.8100\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 0s 787us/step - loss: 0.4686 - binary_accuracy: 0.8300\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 0s 705us/step - loss: 0.4604 - binary_accuracy: 0.8300\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 0s 714us/step - loss: 0.4526 - binary_accuracy: 0.8500\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 0s 572us/step - loss: 0.4448 - binary_accuracy: 0.8800\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 0s 787us/step - loss: 0.4380 - binary_accuracy: 0.8900\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 0s 644us/step - loss: 0.4318 - binary_accuracy: 0.9050\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 0s 572us/step - loss: 0.4267 - binary_accuracy: 0.9050\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 0s 792us/step - loss: 0.4224 - binary_accuracy: 0.9050\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 0s 647us/step - loss: 0.4184 - binary_accuracy: 0.9000\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 0s 572us/step - loss: 0.4152 - binary_accuracy: 0.9000\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 0s 725us/step - loss: 0.4124 - binary_accuracy: 0.9000\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 0s 699us/step - loss: 0.4096 - binary_accuracy: 0.9000\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 0s 749us/step - loss: 0.4075 - binary_accuracy: 0.9000\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 0s 714us/step - loss: 0.4049 - binary_accuracy: 0.9000\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 0s 572us/step - loss: 0.4028 - binary_accuracy: 0.9000\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 0s 647us/step - loss: 0.4005 - binary_accuracy: 0.9000\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 0s 572us/step - loss: 0.3983 - binary_accuracy: 0.9000\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - 0s 715us/step - loss: 0.3967 - binary_accuracy: 0.9000\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - 0s 649us/step - loss: 0.3931 - binary_accuracy: 0.9000\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - 0s 754us/step - loss: 0.3935 - binary_accuracy: 0.9050\n",
      "Epoch 43/100\n",
      "7/7 [==============================] - 0s 572us/step - loss: 0.3917 - binary_accuracy: 0.9050\n",
      "Epoch 44/100\n",
      "7/7 [==============================] - 0s 572us/step - loss: 0.3907 - binary_accuracy: 0.9100\n",
      "Epoch 45/100\n",
      "7/7 [==============================] - 0s 930us/step - loss: 0.3886 - binary_accuracy: 0.9100\n",
      "Epoch 46/100\n",
      "7/7 [==============================] - 0s 647us/step - loss: 0.3860 - binary_accuracy: 0.9100\n",
      "Epoch 47/100\n",
      "7/7 [==============================] - 0s 571us/step - loss: 0.3836 - binary_accuracy: 0.9050\n",
      "Epoch 48/100\n",
      "7/7 [==============================] - 0s 619us/step - loss: 0.3823 - binary_accuracy: 0.9050\n",
      "Epoch 49/100\n",
      "7/7 [==============================] - 0s 934us/step - loss: 0.3802 - binary_accuracy: 0.9050\n",
      "Epoch 50/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3783 - binary_accuracy: 0.9050\n",
      "Epoch 51/100\n",
      "7/7 [==============================] - 0s 930us/step - loss: 0.3765 - binary_accuracy: 0.9050\n",
      "Epoch 52/100\n",
      "7/7 [==============================] - 0s 429us/step - loss: 0.3747 - binary_accuracy: 0.9000\n",
      "Epoch 53/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.3731 - binary_accuracy: 0.9050\n",
      "Epoch 54/100\n",
      "7/7 [==============================] - 0s 643us/step - loss: 0.3715 - binary_accuracy: 0.9050\n",
      "Epoch 55/100\n",
      "7/7 [==============================] - 0s 429us/step - loss: 0.3698 - binary_accuracy: 0.9050\n",
      "Epoch 56/100\n",
      "7/7 [==============================] - 0s 787us/step - loss: 0.3683 - binary_accuracy: 0.9050\n",
      "Epoch 57/100\n",
      "7/7 [==============================] - 0s 649us/step - loss: 0.3667 - binary_accuracy: 0.9050\n",
      "Epoch 58/100\n",
      "7/7 [==============================] - 0s 572us/step - loss: 0.3650 - binary_accuracy: 0.9050\n",
      "Epoch 59/100\n",
      "7/7 [==============================] - 0s 729us/step - loss: 0.3633 - binary_accuracy: 0.9050\n",
      "Epoch 60/100\n",
      "7/7 [==============================] - 0s 656us/step - loss: 0.3616 - binary_accuracy: 0.9050\n",
      "Epoch 61/100\n",
      "7/7 [==============================] - 0s 572us/step - loss: 0.3594 - binary_accuracy: 0.9100\n",
      "Epoch 62/100\n",
      "7/7 [==============================] - 0s 715us/step - loss: 0.3575 - binary_accuracy: 0.9100\n",
      "Epoch 63/100\n",
      "7/7 [==============================] - 0s 803us/step - loss: 0.3556 - binary_accuracy: 0.9050\n",
      "Epoch 64/100\n",
      "7/7 [==============================] - 0s 786us/step - loss: 0.3532 - binary_accuracy: 0.9050\n",
      "Epoch 65/100\n",
      "7/7 [==============================] - 0s 572us/step - loss: 0.3510 - binary_accuracy: 0.9050\n",
      "Epoch 66/100\n",
      "7/7 [==============================] - 0s 854us/step - loss: 0.3481 - binary_accuracy: 0.9050\n",
      "Epoch 67/100\n",
      "7/7 [==============================] - 0s 647us/step - loss: 0.3448 - binary_accuracy: 0.9100\n",
      "Epoch 68/100\n",
      "7/7 [==============================] - 0s 572us/step - loss: 0.3417 - binary_accuracy: 0.9100\n",
      "Epoch 69/100\n",
      "7/7 [==============================] - 0s 778us/step - loss: 0.3376 - binary_accuracy: 0.9100\n",
      "Epoch 70/100\n",
      "7/7 [==============================] - 0s 787us/step - loss: 0.3338 - binary_accuracy: 0.9100\n",
      "Epoch 71/100\n",
      "7/7 [==============================] - 0s 647us/step - loss: 0.3291 - binary_accuracy: 0.9150\n",
      "Epoch 72/100\n",
      "7/7 [==============================] - 0s 572us/step - loss: 0.3248 - binary_accuracy: 0.9050\n",
      "Epoch 73/100\n",
      "7/7 [==============================] - 0s 715us/step - loss: 0.3195 - binary_accuracy: 0.9050\n",
      "Epoch 74/100\n",
      "7/7 [==============================] - 0s 842us/step - loss: 0.3140 - binary_accuracy: 0.9050\n",
      "Epoch 75/100\n",
      "7/7 [==============================] - 0s 787us/step - loss: 0.3085 - binary_accuracy: 0.9100\n",
      "Epoch 76/100\n",
      "7/7 [==============================] - 0s 429us/step - loss: 0.3020 - binary_accuracy: 0.9050\n",
      "Epoch 77/100\n",
      "7/7 [==============================] - 0s 572us/step - loss: 0.2951 - binary_accuracy: 0.9050\n",
      "Epoch 78/100\n",
      "7/7 [==============================] - 0s 787us/step - loss: 0.2891 - binary_accuracy: 0.9050\n",
      "Epoch 79/100\n",
      "7/7 [==============================] - 0s 790us/step - loss: 0.2827 - binary_accuracy: 0.9050\n",
      "Epoch 80/100\n",
      "7/7 [==============================] - 0s 571us/step - loss: 0.2765 - binary_accuracy: 0.9100\n",
      "Epoch 81/100\n",
      "7/7 [==============================] - 0s 572us/step - loss: 0.2703 - binary_accuracy: 0.9050\n",
      "Epoch 82/100\n",
      "7/7 [==============================] - 0s 790us/step - loss: 0.2644 - binary_accuracy: 0.9050\n",
      "Epoch 83/100\n",
      "7/7 [==============================] - 0s 772us/step - loss: 0.2588 - binary_accuracy: 0.9050\n",
      "Epoch 84/100\n",
      "7/7 [==============================] - 0s 572us/step - loss: 0.2537 - binary_accuracy: 0.9050\n",
      "Epoch 85/100\n",
      "7/7 [==============================] - 0s 787us/step - loss: 0.2485 - binary_accuracy: 0.9050\n",
      "Epoch 86/100\n",
      "7/7 [==============================] - 0s 722us/step - loss: 0.2440 - binary_accuracy: 0.9050\n",
      "Epoch 87/100\n",
      "7/7 [==============================] - 0s 713us/step - loss: 0.2397 - binary_accuracy: 0.9050\n",
      "Epoch 88/100\n",
      "7/7 [==============================] - 0s 714us/step - loss: 0.2359 - binary_accuracy: 0.9050\n",
      "Epoch 89/100\n",
      "7/7 [==============================] - 0s 572us/step - loss: 0.2328 - binary_accuracy: 0.9050\n",
      "Epoch 90/100\n",
      "7/7 [==============================] - 0s 829us/step - loss: 0.2296 - binary_accuracy: 0.9050\n",
      "Epoch 91/100\n",
      "7/7 [==============================] - 0s 512us/step - loss: 0.2271 - binary_accuracy: 0.9050\n",
      "Epoch 92/100\n",
      "7/7 [==============================] - 0s 714us/step - loss: 0.2252 - binary_accuracy: 0.9050\n",
      "Epoch 93/100\n",
      "7/7 [==============================] - 0s 715us/step - loss: 0.2234 - binary_accuracy: 0.9050\n",
      "Epoch 94/100\n",
      "7/7 [==============================] - 0s 792us/step - loss: 0.2228 - binary_accuracy: 0.9050\n",
      "Epoch 95/100\n",
      "7/7 [==============================] - 0s 714us/step - loss: 0.2206 - binary_accuracy: 0.9100\n",
      "Epoch 96/100\n",
      "7/7 [==============================] - 0s 857us/step - loss: 0.2195 - binary_accuracy: 0.9100\n",
      "Epoch 97/100\n",
      "7/7 [==============================] - 0s 715us/step - loss: 0.2185 - binary_accuracy: 0.9100\n",
      "Epoch 98/100\n",
      "7/7 [==============================] - 0s 719us/step - loss: 0.2176 - binary_accuracy: 0.9050\n",
      "Epoch 99/100\n",
      "7/7 [==============================] - 0s 950us/step - loss: 0.2169 - binary_accuracy: 0.9050\n",
      "Epoch 100/100\n",
      "7/7 [==============================] - 0s 715us/step - loss: 0.2160 - binary_accuracy: 0.9050\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4.** Evaluate the model using the test data and generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9050\n",
      "Test Accuracy: 0.8800\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the network\n",
    "train_accuracy = history.history[\"binary_accuracy\"][-1]\n",
    "result = model.evaluate(X_test,y_test, verbose=0)\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {result[1]:.4f}\") \n",
    "\n",
    "# Generate predictions\n",
    "predictions = model.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
