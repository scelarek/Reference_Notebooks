{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3)\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# Add some noise to the data to make it harder to classify\n",
    "random_state = np.random.RandomState(10)\n",
    "n_samples, n_features = X.shape\n",
    "X = X + (random_state.randn(n_samples, n_features) * 10)\n",
    "\n",
    "# Train-Test-Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.5, random_state=1)\n",
    "\n",
    "#Transform data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "Brain Station\\Reference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "- **Intuition behind test**: Logistic regression is a statistical model that uses a logistic function to model a binary dependent variable. It's a way to predict the probability of a certain event happening, which makes it a very good fit for binary classification problems.\n",
    "\n",
    "- **Use case for test**: Logistic regression is used when the dependent variable is binary. It's widely used for binary classification problems like spam detection, churn prediction, or health diagnosis.\n",
    "\n",
    "- **Intuition for using it for classification**: Logistic regression outputs probabilities. If the probability is greater than 0.5, it assigns the instance to the positive class, otherwise it assigns it to the negative class.\n",
    "\n",
    "- **Intuition for using it for regression**: Logistic regression is not typically used for regression tasks as it's designed for binary classification tasks.\n",
    "\n",
    "- **How to code it**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **The most important hyperparameters to iterate through**: `C` (Inverse of regularization strength), `penalty` (Specifies the norm used in the penalization), `solver` (Algorithm to use in the optimization problem)\n",
    "\n",
    "- **Code for iterating through one example of a hyperparameter**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=LogisticRegression(),\n",
       "             param_grid={&#x27;C&#x27;: [0.001, 0.01, 0.1, 1, 10, 100, 1000]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=LogisticRegression(),\n",
       "             param_grid={&#x27;C&#x27;: [0.001, 0.01, 0.1, 1, 10, 100, 1000]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=LogisticRegression(),\n",
       "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }\n",
    "clf = GridSearchCV(LogisticRegression(penalty='l2'), param_grid)\n",
    "GridSearchCV(cv=None,\n",
    "            estimator=LogisticRegression(C=1.0, intercept_scaling=1,   \n",
    "              dual=False, fit_intercept=True, penalty='l2', tol=0.0001),\n",
    "            param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Assumptions of the algorithm**: Logistic regression assumes that there is a linear relationship between the logit of the response and the predictors, requires the dependent variable to be binary and assumes no error in measurement.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## K-Nearest Neighbors\n",
    "\n",
    "- **Intuition behind test**: K-Nearest Neighbors (KNN) is a type of instance-based learning where the function is only approximated locally and all computation is deferred until function evaluation.\n",
    "\n",
    "- **Use case for test**: KNN can be used for both classification and regression predictive problems. However, it is more widely used in classification problems in the industry.\n",
    "\n",
    "- **Intuition for using it for classification**: KNN works by finding the distances between a query and all the examples in the data, selecting the specified number examples (K) closest to the query, then votes for the most frequent label (in the case of classification) or averages the labels (in the case of regression).\n",
    "\n",
    "- **Intuition for using it for regression**: For regression, KNN takes the average of the numerical target of the K nearest neighbors.\n",
    "\n",
    "- **How to code it**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 2, 2, 0, 1, 1, 2, 1, 2, 2, 1, 1, 2, 2, 1, 2, 0, 2, 1, 1, 1,\n",
       "       2, 2, 1, 1, 0, 1, 0, 0, 0, 1, 2, 0, 0, 2, 1, 2, 0, 2, 0, 0, 2, 0,\n",
       "       0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **The most important hyperparameters to iterate through**: `n_neighbors` (Number of neighbors to use), `weights` (Weight function used in prediction), `p` (Power parameter for the Minkowski metric)\n",
    "\n",
    "- **Code for iterating through one example of a hyperparameter**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3830891156.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[10], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    param_grid =\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid =\n",
    "\n",
    "{'n_neighbors': [3, 5, 7, 9, 11]}\n",
    "clf = GridSearchCV(KNeighborsClassifier(), param_grid)\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Assumptions of the algorithm**: KNN assumes that similar things exist in close proximity. In other words, similar things are near to each other.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Decision Trees\n",
    "\n",
    "- **Intuition behind test**: Decision Trees is a type of algorithm that makes decisions based on conditions. It's like playing a game of 20 questions to predict the class or value of the target variable.\n",
    "\n",
    "- **Use case for test**: Decision Trees are used for both classification and regression tasks. They are widely used in customer segmentation, detection of fraudulent transactions, or prediction of diseases.\n",
    "\n",
    "- **Intuition for using it for classification**: The tree is constructed in a way that the most important features appear at the top of the tree. It splits the data into subsets based on the feature that provides the most information gain. This process is repeated recursively until it makes a prediction for every subset.\n",
    "\n",
    "- **Intuition for using it for regression**: Decision tree regression observes features of an object and trains a model in the structure of a tree to predict data in the future to produce meaningful continuous output.\n",
    "\n",
    "- **How to code it**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **The most important hyperparameters to iterate through**: `max_depth` (The maximum depth of the tree), `min_samples_split` (The minimum number of samples required to split an internal node), `min_samples_leaf` (The minimum number of samples required to be at a leaf node)\n",
    "\n",
    "- **Code for iterating through one example of a hyperparameter**:\n",
    "\n",
    "```python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'max_depth': [3, 5, 7, 9, 11]}\n",
    "clf = GridSearchCV(DecisionTreeClassifier(), param_grid)\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Assumptions of the algorithm**: Decision tree algorithm assumes that the training data is noise-free, it assumes that missing values are at random, and the most crucial assumption is that the training set is a sample from the actual population."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Linear Regression\n",
    "\n",
    "- **Intuition behind test**: Linear regression is a statistical model that examines the linear relationship between two (Simple Linear Regression ) or more (Multiple Linear Regression) variables — a dependent variable and independent variable(s).\n",
    "\n",
    "- **Use case for test**: Linear regression is used when we want to predict the value of a variable based on the value of another variable. The variable we want to predict is called the dependent variable (or sometimes, the outcome variable).\n",
    "\n",
    "- **Intuition for using it for classification**: Linear regression is not typically used for classification tasks. It is more suited for estimating values.\n",
    "\n",
    "- **Intuition for using it for regression**: Linear regression creates a model that predicts the dependent variable as a linear function of the independent variables. It finds the line of best fit that minimizes the sum of the residuals.\n",
    "\n",
    "- **How to code it**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **The most important hyperparameters to iterate through**: `fit_intercept` (Whether to calculate the intercept for this model), `normalize` (This parameter is ignored when fit_intercept is set to False)\n",
    "\n",
    "- **Code for iterating through one example of a hyperparameter**: Linear regression does not typically require hyperparameter tuning.\n",
    "\n",
    "- **Assumptions of the algorithm**: Linear regression assumes that there is a linear relationship between the dependent and independent variables, the residuals are normally distributed and have constant variance, and there is no multicollinearity among independent variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Support Vector Machines (SVMs)\n",
    "\n",
    "- **Intuition behind test**: SVM is a supervised machine learning algorithm which can be used for both classification or regression challenges. However, it is mostly used in classification problems. In the SVM algorithm, we plot each data item as a point in n-dimensional space (where n is the number of features you have) with the value of each feature being the value of a particular coordinate.\n",
    "\n",
    "- **Use case for test**: SVMs are helpful in text and hypertext categorization, classification of images, and in the biological and other sciences.\n",
    "\n",
    "- **Intuition for using it for classification**: SVMs are based on the idea of finding a hyperplane that best separates the features into different classes.\n",
    "\n",
    "- **Intuition for using it for regression**: In the case of regression, SVMs find the hyperplane that deviates from the most of the data points by no more than a certain amount, and for the rest of the data points, tries to minimize the deviation.\n",
    "\n",
    "- **How to code it**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import svm\n",
    "\n",
    "model = svm.SVC()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **The most important hyperparameters to iterate through**: `C` (Penalty parameter C of the error term), `kernel` (Specifies the kernel type to be used in the algorithm), `gamma` (Kernel coefficient for 'rbf', 'poly' and 'sigmoid')\n",
    "\n",
    "- **Code for iterating through one example of a hyperparameter**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001], 'kernel': ['rbf']} \n",
    "clf = GridSearchCV(svm.SVC(), param_grid)\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Assumptions of the algorithm**: SVMs assume that the data it works with is in a specific format. Namely, that all of the input features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "- **Intuition behind test**: PCA is a dimensionality reduction technique that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.\n",
    "\n",
    "- **Use case for test**: PCA is used in exploratory data analysis and for making predictive models. It is commonly used for dimensionality reduction by projecting each data point onto only the first few principal components to obtain lower-dimensional data while preserving as much of the data's variation as possible.\n",
    "\n",
    "- **Intuition for using it for classification**: PCA itself is an unsupervised method and doesn't use any class label information. However, the transformed features (principal components) from PCA can be used for classification tasks.\n",
    "\n",
    "- **Intuition for using it for regression**: Similarly, PCA doesn't directly apply to regression tasks, but the principal components can be used as predictors in a regression model.\n",
    "\n",
    "- **How to code it**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **The most important hyperparameters to iterate through**: `n_components` (Number of components to keep)\n",
    "\n",
    "- **Code for iterating through one example of a hyperparameter**: PCA typically doesn't require hyperparameter tuning.\n",
    "\n",
    "- **Assumptions of the algorithm**: PCA assumes that the principal components are a linear combination of the original features, the components are orthogonal, and the most important component is the one that explains the most variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Naive Bayes\n",
    "\n",
    "- **Intuition behind test**: Naive Bayes classifiers are a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with strong (naive) independence assumptions between the features.\n",
    "\n",
    "- **Use case for test**: Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem. They are often used for text classification, spam filtering, and recommendation systems.\n",
    "\n",
    "- **Intuition for using it for classification**: Naive Bayes is a probabilistic classifier, meaning it predicts on the basis of the probability of an object. It uses Bayes' Theorem, which is based on the concept of conditional probability.\n",
    "\n",
    "- **Intuition for using it for regression**: Naive Bayes is not typically used for regression tasks as it's a probabilistic classifier and works based on the assumption of independence among predictors.\n",
    "\n",
    "- **How to code it**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **The most important hyperparameters to iterate through**: Naive Bayes typically doesn't have hyperparameters that need tuning, but some implementations like `BernoulliNB` and `MultinomialNB` have a `alpha` parameter which is a smoothing parameter.\n",
    "\n",
    "- **Code for iterating through one example of a hyperparameter**: Naive Bayes typically doesn't require hyperparameter tuning.\n",
    "\n",
    "- **Assumptions of the algorithm**: Naive Bayes assumes that all features are independent from each other and each one contributes independently to the probability of the outcome. This is a 'naive' assumption because it's rarely true in real-world scenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
