{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure, here is a Markdown cheatsheet for the concepts you mentioned:\n",
    "\n",
    "## Optimization\n",
    "- **Definition**: Optimization in machine learning is the process of adjusting the parameters of a model to minimize the model's error rate and improve its predictive accuracy.\n",
    "- **Use case and Intuition**: Optimization is used in virtually every machine learning algorithm to find the best model parameters that minimize the cost function.\n",
    "- **Formula**: Varies depending on the specific optimization algorithm used (e.g., Gradient Descent, Stochastic Gradient Descent, Adam, etc.)\n",
    "- **Example**: Training a neural network using backpropagation and gradient descent.\n",
    "\n",
    "## Cost Function\n",
    "- **Definition**: A cost function (or loss function) quantifies the error between predicted values and expected values and presents it in the form of a single real number.\n",
    "- **Use case and Intuition**: Used to estimate how well the model is performing during the training phase. The goal is to minimize this cost function.\n",
    "- **Formula**: Varies depending on the specific cost function used (e.g., Mean Squared Error for regression, Cross-Entropy for classification, etc.)\n",
    "- **Example**: In linear regression, the cost function is the Mean Squared Error (MSE) of the predicted values versus the actual values.\n",
    "\n",
    "## Overfitting and Variance\n",
    "- **Definition**: Overfitting occurs when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data.\n",
    "- **Use case and Intuition**: Overfitting leads to high variance in the predictions for new data, and is often a result of an overly complex model.\n",
    "- **Formula**: No specific formula, but can be diagnosed using techniques like cross-validation.\n",
    "- **Example**: A decision tree that is grown too deep tends to overfit the training data and performs poorly on unseen data.\n",
    "\n",
    "## Underfitting and Bias\n",
    "- **Definition**: Underfitting occurs when a model cannot capture the underlying trend of the data. \n",
    "- **Use case and Intuition**: Underfitting leads to high bias in the predictions, and is often a result of an overly simple model.\n",
    "- **Formula**: No specific formula, but can be diagnosed using techniques like cross-validation.\n",
    "- **Example**: A linear regression model trying to fit non-linear data would typically underfit.\n",
    "\n",
    "## Distance\n",
    "- **Definition**: Distance measures are used in machine learning to quantify the similarity or dissimilarity between data points.\n",
    "- **Use case and Intuition**: Used in clustering, anomaly detection, and k-nearest neighbors algorithms.\n",
    "- **Formula**: Varies depending on the specific distance measure used (e.g., Euclidean distance, Manhattan distance, etc.)\n",
    "- **Example**: In k-nearest neighbors, the Euclidean distance can be used to find the k closest points to a given test point.\n",
    "\n",
    "## Bootstrapping\n",
    "- **Definition**: Bootstrapping is a resampling technique used to estimate statistics on a population by sampling a dataset with replacement.\n",
    "- **Use case and Intuition**: It is used when the theoretical distribution of a statistic of interest is complicated or unknown.\n",
    "- **Formula**: No specific formula, but involves generating 'n' new bootstrap samples of the dataset.\n",
    "- **Example**: Random Forests use bootstrapping when training each decision tree.\n",
    "\n",
    "## Class Imbalance\n",
    "- **Definition**: Class imbalance refers to a situation where the classes are not represented equally in a classification problem.\n",
    "- **Use case and Intuition**: It is a common problem in machine learning, especially in classification problems, where the classes are not equally represented.\n",
    "- **Formula**: No specific formula, but can be addressed using techniques like resampling, SMOTE, etc.\n",
    "- **Example**: In fraud detection, the number of fraud cases (positive class) is often\n",
    "\n",
    "I'm sorry, but I encountered an error while trying to fetch relevant papers on these topics. However, I can continue providing the information you requested.\n",
    "\n",
    "## Parameters\n",
    "- **Definition**: Parameters are the internal variables that the model learns during the training process.\n",
    "- **Use case and Intuition**: Parameters are learned from the training data and are used to make predictions on new data.\n",
    "- **Formula**: Varies depending on the specific model used (e.g., weights in a neural network, coefficients in a linear regression, etc.)\n",
    "- **Example**: In a linear regression model, the coefficients are the parameters.\n",
    "\n",
    "## Hyperparameters\n",
    "- **Definition**: Hyperparameters are the configuration variables that are set before the training process begins.\n",
    "- **Use case and Intuition**: Hyperparameters control the learning process and are not learned from the data. They are often set using trial and error or techniques like grid search or random search.\n",
    "- **Formula**: No specific formula, but involves setting values like learning rate, number of hidden layers in a neural network, etc.\n",
    "- **Example**: In a neural network, the learning rate and the number of hidden layers are hyperparameters.\n",
    "\n",
    "## Training, Test, and Validation Sets\n",
    "- **Definition**: These are subsets of your dataset used to train and evaluate your model.\n",
    "- **Use case and Intuition**: The training set is used to train the model, the validation set is used to tune hyperparameters and make decisions on the model, and the test set is used to evaluate the final model.\n",
    "- **Formula**: No specific formula, but involves splitting your dataset into these three subsets.\n",
    "- **Example**: A common split is 70% of the data for training, 15% for validation, and 15% for testing.\n",
    "\n",
    "I hope this information is helpful! If you have any other questions or need further clarification, feel free to ask."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Concepts\n",
    "\n",
    "## Regression vs Classification\n",
    "- **Definition**: Regression and Classification are two types of supervised learning techniques. Regression predicts a continuous output variable, while Classification predicts a categorical output variable.\n",
    "- **Use Case and Intuition**: Regression is used when the output variable is a real or continuous value, such as \"salary\" or \"weight\". Classification is used when the output variable is a category, such as \"spam\" or \"not spam\".\n",
    "- **Formula**: Not applicable as these are broad concepts, not specific formulas.\n",
    "- **Examples**: Predicting the price of a house based on its features is a regression problem. Predicting whether an email is spam or not is a classification problem.\n",
    "\n",
    "## Probability Density Function\n",
    "- **Definition**: A Probability Density Function (PDF) is a function that describes the likelihood of a random variable to take on the value x.\n",
    "- **Use Case and Intuition**: PDFs are used to specify the probability of the random variable falling within a particular range of values.\n",
    "- **Formula**: The PDF of a continuous random variable X is an integral of the variable's distribution. For a discrete random variable, the PDF is the probability that the variable takes the value x, i.e., P(X=x).\n",
    "- **Examples**: The normal distribution, the exponential distribution, and the beta distribution all have PDFs.\n",
    "\n",
    "## Different Types of Probability Distributions\n",
    "- **Definition**: A probability distribution is a statistical function that describes all the possible values and likelihoods that a random variable can take within a given range.\n",
    "- **Use Case and Intuition**: Probability distributions are used in statistics and data science to model phenomena and facilitate hypothesis testing.\n",
    "- **Formula**: Each distribution has its own formula. For example, the formula for the normal distribution's PDF is:\n",
    "    $$ f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{ -\\frac{(x-\\mu)^2}{2\\sigma^2} } $$\n",
    "    where \\(\\mu\\) is the mean and \\(\\sigma\\) is the standard deviation.\n",
    "- **Examples**: Examples of probability distributions include the Normal Distribution, Binomial Distribution, Poisson Distribution, etc.\n",
    "\n",
    "## Multicollinearity\n",
    "- **Definition**: Multicollinearity is a statistical phenomenon in which predictor variables in a regression model are highly correlated.\n",
    "- **Use Case and Intuition**: In the presence of multicollinearity, it becomes difficult to determine the effect of each predictor variable on the response variable due to the interdependencies among the predictor variables.\n",
    "- **Formula**: Multicollinearity is often detected using the Variance Inflation Factor (VIF).\n",
    "- **Examples**: If you are trying to predict a person's weight based on their height and age, and you have another variable that is a person's height in meters, then height and height in meters would be highly correlated, causing multicollinearity.\n",
    "\n",
    "## Variance and Expected Value\n",
    "- **Definition**: The expected value of a random variable gives a measure of the center of the distribution. The variance measures the dispersion of the data.\n",
    "- **Use Case and Intuition**: The expected value provides a measure of central tendency, while the variance provides a measure of spread.\n",
    "- **Formula**: The formula for expected value (E) and variance (Var) for a random variable X are:\n",
    "    $$ E[X] = \\sum xP(X=x) \\quad \\text{for discrete variables} $$\n",
    "    $$ E[X] = \\int xf(x)dx \\quad \\text{for continuous variables} $$\n",
    "    $$ Var[X] = E[(X - E[X])^2] $$\n",
    "- **Examples**: If you roll a fair six-sided die, the expected value is 3.\n",
    "\n",
    "## Variance and Expected Value\n",
    "- **Definition**: The expected value of a random variable gives a measure of the center of the distribution. The variance measures the dispersion of the data.\n",
    "- **Use Case and Intuition**: The expected value provides a measure of central tendency, while the variance provides a measure of spread.\n",
    "- **Formula**: The formula for expected value (E) and variance (Var) for a random variable X are:\n",
    "    $$ E[X] = \\sum xP(X=x) \\quad \\text{for discrete variables} $$\n",
    "    $$ E[X] = \\int xf(x)dx \\quad \\text{for continuous variables} $$\n",
    "    $$ Var[X] = E[(X - E[X])^2] $$\n",
    "- **Examples**: If you roll a fair six-sided die, the expected value is 3.5 and the variance is approximately 2.92.\n",
    "\n",
    "## Model Evaluation\n",
    "- **Definition**: Model evaluation involves assessing the performance of a predictive model using a set of metrics.\n",
    "- **Use Case and Intuition**: Model evaluation is used to determine how well a model is performing and to compare the performance of different models.\n",
    "- **Formula**: There are many different metrics for model evaluation, and the choice of metric depends on the task. For example, accuracy, precision, recall, and F1 score are commonly used for classification tasks, while mean squared error, root mean squared error, and R^2 are commonly used for regression tasks.\n",
    "- **Examples**: Evaluating a model that predicts whether an email is spam or not using accuracy, precision, recall, and F1 score.\n",
    "\n",
    "## Sum of the Mean Squared Residual\n",
    "- **Definition**: The sum of the mean squared residuals (also known as the residual sum of squares) is a measure of the discrepancy between the data and an estimation model. A small RSS indicates a tight fit of the model to the data.\n",
    "- **Use Case and Intuition**: It is used in regression analysis to measure the amount of variance in the data that is not explained by the model.\n",
    "- **Formula**: The formula for the residual sum of squares (RSS) is:\n",
    "    $$ RSS = \\sum (y_i - f(x_i))^2 $$\n",
    "    where \\(y_i\\) are the observed data points, and \\(f(x_i)\\) are the points predicted by the model.\n",
    "- **Examples**: In linear regression, the line of best fit is chosen to be the one that minimizes the residual sum of squares.\n",
    "\n",
    "For more detailed information, you can refer to these papers:\n",
    "1. [Comparison of Estimation Accuracy of EKF, UKF and PF Filters](https://dx.doi.org/10.1515/aon-2016-0005)\n",
    "2. [Clinical and laboratory data, radiological structured report findings and quantitative evaluation of lung involvement on baseline chest CT in COVID-19 patients to predict prognosis](https://dx.doi.org/10.1007/s11547-020-01293-w)\n",
    "3. [Model-Based Deep Learning PET Image Reconstruction Using Forward–Backward Splitting Expectation–Maximization](https://dx.doi.org/10.1109/TRPMS.2020.3004408)\n",
    "4. [The Effect of Zirconium Dioxide (ZrO2) Nanoparticles Addition on the Mechanical Parameters of Polymethyl Methacrylate (PMMA): A Systematic Review and Meta-Analysis of Experimental Studies](https://dx.doi.org/10.3390/polym14051047)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
