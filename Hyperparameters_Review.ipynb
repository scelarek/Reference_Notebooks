{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Concepts in Machine Learning\n",
    "\n",
    "## Regularization\n",
    "- **Definition**: Regularization is a technique used to prevent overfitting in a machine learning model by adding a penalty term to the loss function. The penalty term discourages the model from learning overly complex patterns in the training data that may not generalize well to unseen data.\n",
    "- **Use Case and Intuition**: Regularization is used when a model is overfitting the training data, i.e., it performs well on the training data but poorly on the test data. The intuition behind regularization is to keep the weights of the model small, which makes the model simpler and less likely to overfit.\n",
    "- **Formula**: In the case of L2 regularization (also known as Ridge regression), the penalty term is the sum of the squares of the model weights, multiplied by the regularization parameter (Î»). In the case of L1 regularization (also known as Lasso regression), the penalty term is the sum of the absolute values of the model weights, multiplied by the regularization parameter.\n",
    "- **Code Example**:\n",
    "    ```python\n",
    "    from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "    # Ridge regression\n",
    "    ridge = Ridge(alpha=1.0)\n",
    "    ridge.fit(X_train, y_train)\n",
    "\n",
    "    # Lasso regression\n",
    "    lasso = Lasso(alpha=1.0)\n",
    "    lasso.fit(X_train, y_train)\n",
    "    ```\n",
    "- **Assumptions and Cautions**: Regularization assumes that smaller weights lead to simpler and more generalizable models. However, the choice of the regularization parameter is critical and needs to be carefully tuned. Also, L1 regularization can lead to sparse solutions where some weights are exactly zero, while L2 regularization tends to distribute the weights evenly.\n",
    "- **Interpretation**: Models with regularization are interpreted in the same way as their non-regularized counterparts. However, it's important to note that the weights of the regularized models are shrunk towards zero, which can affect the interpretation of the feature importance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Transformation\n",
    "- **Definition**: Transformation is the process of changing the distribution or relationship of a feature using a mathematical function.\n",
    "- **Use Case and Intuition**: Transformation is used when the distribution of the feature is not suitable for the machine learning algorithm. For example, some algorithms assume that the features are normally distributed. The intuition is to make the distribution of the feature more suitable for the machine learning algorithm.\n",
    "- **Formula**: Common transformations include log transformation, square root transformation, and inverse transformation.\n",
    "- **Code Example**:\n",
    "    ```python\n",
    "    import numpy as np\n",
    "\n",
    "    # Log transformation\n",
    "    X_log = np.log(X + 1)\n",
    "\n",
    "    # Square root transformation\n",
    "    X_sqrt = np.sqrt(X)\n",
    "\n",
    "    # Inverse transformation\n",
    "    X_inv = 1 / (X + 1)\n",
    "    ```\n",
    "- **Assumptions and Cautions**: Transformation assumes that the features are numerical and continuous. Also, the choice of the transformation depends on the distribution of the feature and the assumptions of the machine learning algorithm.\n",
    "- **Interpretation**: After transformation, the distribution of the feature will be different, which can affect the interpretation of the feature. For example, after log transformation, a change in the feature corresponds to a percentage change in the target variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Stratification\n",
    "- **Definition**: Stratification is the process of dividing the data into homogeneous subgroups before sampling. The strata should be mutually exclusive: every element in the population must be assigned to only one stratum. The strata should also be collectively exhaustive: no population element can be excluded.\n",
    "- **Use Case and Intuition**: Stratification is used when the target variable is imbalanced, i.e., one class has many more examples than the other class. The intuition is to ensure that each class is adequately represented in the training and test sets.\n",
    "- **Formula**: Stratification does not have a specific formula. It involves dividing the data into subgroups based on the target variable.\n",
    "- **Code Example**:\n",
    "    ```python\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "    sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "    for train_index, test_index in sss.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "    ```\n",
    "- **Assumptions and Cautions**: Stratification assumes that the distribution of the target variable is important to maintain in the training and test sets. However, stratification can lead to biased estimates if the strata are not defined correctly.\n",
    "- **Interpretation**: Stratification does not affect the interpretation of the model, but it can improve the reliability of the performance estimates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Cross-Validation\n",
    "- **Definition**: Cross-validation is a technique for assessing how the results of a statistical analysis will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice.\n",
    "- **Use Case and Intuition**: Cross-validation is used to estimate the performance of a machine learning model on unseen data. The intuition is to divide the data into training and test sets multiple times and to average the performance across all splits.\n",
    "- **Formula**: Cross-validation does not have a specific formula. It involves dividing the data into training and test sets multiple times and averaging the performance.\n",
    "- **Code Example**:\n",
    "    ```python\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    clf = RandomForestClassifier(random_state=42)\n",
    "    scores = cross_val_score(clf, X, y, cv=5)\n",
    "    print(\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Cross-Validation\n",
    "- **Definition**: Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample. The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into.\n",
    "- **Use Case and Intuition**: Cross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data. That is, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model.\n",
    "- **Formula**: Cross-validation does not have a specific formula. It involves dividing the data into k subsets of approximately equal size. The model is trained on k-1 subsets and tested on the remaining subset. This process is repeated k times, with each subset serving as the test set once.\n",
    "- **Code Example**:\n",
    "    ```python\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    clf = RandomForestClassifier(random_state=42)\n",
    "    scores = cross_val_score(clf, X, y, cv=5)\n",
    "    print(\"Cross-validation scores: \", scores)\n",
    "    ```\n",
    "- **Assumptions and Cautions**: Cross-validation assumes that the data is independently and identically distributed. It may not be appropriate if there is a temporal or spatial structure in the data. Also, the choice of k is important. A small k may lead to a high variance in the performance estimate, while a large k may be computationally expensive.\n",
    "- **Interpretation**: The performance of the model is usually taken as the average performance over the k folds. A high variance in the performance across folds may indicate that the model is sensitive to the specific partitioning of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Feature Engineering\n",
    "- **Definition**: Feature engineering is the process of creating new features or modifying existing features to improve the performance of a machine learning model.\n",
    "- **Use Case and Intuition**: Feature engineering is used when the existing features are not sufficient to capture the underlying pattern in the data. The intuition is to create features that have a strong relationship with the target variable or that interact well with the machine learning algorithm.\n",
    "- **Formula**: Feature engineering does not have a specific formula. It can involve various techniques such as binning, polynomial features, interaction features, and domain-specific transformations.\n",
    "- **Code Example**:\n",
    "    ```python\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "    poly = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    ```\n",
    "- **Assumptions and Cautions**: Feature engineering assumes that the new or modified features will improve the performance of the machine learning model. However, it can lead to overfitting if the features are too complex or if they are not relevant to the target variable.\n",
    "- **Interpretation**: Features created through feature engineering are interpreted in the same way as the original features. However, the interpretation can be less intuitive, especially for complex transformations or interactions.\n",
    "\n",
    "For more detailed information, you can refer to these papers:\n",
    "1. [Classification of Small Lesions in Breast MRI: Evaluating The Role of Dynamically Extracted Texture Features Through Feature Selection.](https://dx.doi.org/10.5405/jmbe.1183) \n",
    "2. [Ensemble learning based automatic detection of tuberculosis in chest X-ray images using hybrid feature descriptors](https://dx.doi.org/10.1007/s13246-020-00966-0)\n",
    "3. [Enhancing EEG-Based Classification of Depression Patients Using Spatial Information](https://dx.doi.org/10.1109/TNSRE.2021.3059429)\n",
    "4. [Locally Linear\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Cross-Validation\n",
    "- **Definition**: Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample. The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into.\n",
    "- **Use Case and Intuition**: Cross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data. That is, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model.\n",
    "- **Formula**: Cross-validation does not have a specific formula. It involves dividing the data into k subsets of approximately equal size. The model is trained on k-1 subsets and tested on the remaining subset. This process is repeated k times, with each subset serving as the test set once.\n",
    "- **Code Example**:\n",
    "    ```python\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    clf = RandomForestClassifier(random_state=42)\n",
    "    scores = cross_val_score(clf, X, y, cv=5)\n",
    "    print(\"Cross-validation scores: \", scores)\n",
    "    ```\n",
    "- **Assumptions and Cautions**: Cross-validation assumes that the data is independently and identically distributed. It may not be appropriate if there is a temporal or spatial structure in the data. Also, the choice of k is important. A small k may lead to a high variance in the performance estimate, while a large k may be computationally expensive.\n",
    "- **Interpretation**: The performance of the model is usually taken as the average performance over the k folds. A high variance in the performance across folds may indicate that the model is sensitive to the specific partitioning of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Feature Engineering\n",
    "- **Definition**: Feature engineering is the process of creating new features or modifying existing features to improve the performance of a machine learning model.\n",
    "- **Use Case and Intuition**: Feature engineering is used when the existing features are not sufficient to capture the underlying pattern in the data. The intuition is to create features that have a strong relationship with the target variable or that interact well with the machine learning algorithm.\n",
    "- **Formula**: Feature engineering does not have a specific formula. It can involve various techniques such as binning, polynomial features, interaction features, and domain-specific transformations.\n",
    "- **Code Example**:\n",
    "    ```python\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "    poly = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    ```\n",
    "- **Assumptions and Cautions**: Feature engineering assumes that the new or modified features will improve the performance of the machine learning model. However, it can lead to overfitting if the features are too complex or if they are not relevant to the target variable.\n",
    "- **Interpretation**: Features created through feature engineering are interpreted in the same way as the original features. However, the interpretation can be less intuitive, especially for complex transformations or interactions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For more detailed information, you can refer to these papers:\n",
    "1. [Feature Engineering and Selection: A Practical Approach for Predictive Models](https://dx.doi.org/10.1080/00031305.2020.1790217)\n",
    "2. [Machine Learning and feature engineering-based study into sarcasm and irony classification with application to cyberbullying detection](https://dx.doi.org/10.1016/j.ipm.2021.102600)\n",
    "3. [Intrusion Detection System for Internet of Things Based on Temporal Convolution Neural Network and Efficient Feature Engineering](https://dx.doi.org/10.1155/2020/6689134)\n",
    "4. [CNFE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
