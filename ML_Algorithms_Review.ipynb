{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Regression and Classification Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3)\n",
    "\n",
    "#Transform data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Logistic Regression](#toc1_1_)    \n",
    "  - [K-Nearest Neighbors](#toc1_2_)    \n",
    "  - [Decision Trees](#toc1_3_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Logistic Regression](#toc0_)\n",
    "\n",
    "- **Intuition behind test**: Logistic regression is a statistical model that uses a logistic function to model a binary dependent variable. It's a way to predict the probability of a certain event happening, which makes it a very good fit for binary classification problems.\n",
    "\n",
    "- **Use case for test**: Logistic regression is used when the dependent variable is binary. It's widely used for binary classification problems like spam detection, churn prediction, or health diagnosis.\n",
    "\n",
    "- **Classification Intuition**:: Logistic regression outputs probabilities. If the probability is greater than 0.5, it assigns the instance to the positive class, otherwise it assigns it to the negative class. Logistic regression uses the logistic sigmoid function to return a probability value.\n",
    "\n",
    "- **Probability Formula**: The logistic regression function can be written as: \n",
    "    $$ P(Y=1|X) = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1X)}} $$\n",
    "- **Cost Function**: The cost function in logistic regression is the log loss, which can be written as:\n",
    "    $$ J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)}\\log(h_\\theta(x^{(i)})) + (1 - y^{(i)})\\log(1 - h_\\theta(x^{(i)}))] $$\n",
    "\n",
    "- **How to code it**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **The most important hyperparameters to iterate through**:\n",
    "    -   `C`: Inverse of regularization strength; must be a positive float. Smaller values specify stronger regularization.\n",
    "    - `penalty`: Used to specify the norm used in the penalization ('l1', 'l2', 'elasticnet', 'none')\n",
    "    -  `solver` (Algorithm to use in the optimization problem)\n",
    "\n",
    "- **Code for iterating through one example of a hyperparameter**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=LogisticRegression(),\n",
       "             param_grid={&#x27;C&#x27;: [0.001, 0.01, 0.1, 1, 10, 100, 1000]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=LogisticRegression(),\n",
       "             param_grid={&#x27;C&#x27;: [0.001, 0.01, 0.1, 1, 10, 100, 1000]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=LogisticRegression(),\n",
       "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'penalty': ['l1', 'l2']}\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid)\n",
    "grid.fit(X_train, y_train)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Assumptions**:\n",
    "    - Binary logistic regression requires the dependent variable to be binary.\n",
    "    - Requires the observations to be independent of each other.\n",
    "    - Requires little or no multicollinearity among the independent variables.\n",
    "    - Requires the independent variables to be linearly related to the log odds.\n",
    "\n",
    "- **Interpretation of Coefficients**: The coefficients of the logistic regression algorithm can be interpreted as the change in the log odds of the output variable for a one unit change in the input variable. For example, a coefficient of 0.5 would mean that a one unit change in the input variable would result in a 0.5 unit change in the log odds of the output variable.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <a id='toc1_2_'></a>[K-Nearest Neighbors](#toc0_)\n",
    "\n",
    "- **Intuition**: KNN is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until function evaluation. It works by finding a predetermined number of training samples closest in distance to the new point, and predict the label from these.\n",
    "- **Use case**: KNN can be used in both classification and regression predictive problems. However, it is more widely used in classification problems in the industry. K-nearest neighbors algorithm is used for simple classification tasks, where the dataset is small and well-labeled.\n",
    "- **Classification Intuition**: In KNN classification, an object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors.\n",
    "- **Regression Intuition**: In KNN regression, the output is the property value for the object. This value is the average (or median) of the values of k nearest neighbors.\n",
    "- **Probability Formula**: KNN does not provide a formula for probability as it does not have a mathematical model underlying it. It simply calculates the distances to all points and takes the majority vote (for classification) or average/median (for regression) of the k closest points.\n",
    "- **Cost Function**: KNN does not have a cost function as it does not learn a function from the training data. The 'cost' is essentially the computation of distances to all points in the training set.\n",
    "- **How to code it**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Important Hyperparameters**: \n",
    "    - `n_neighbors`: Number of neighbors to use by default for kneighbors queries.\n",
    "    - `weights`: weight function used in prediction. Possible values: 'uniform', 'distance'\n",
    "    - `p` (Power parameter for the Minkowski metric)\n",
    "\n",
    "- **Code for Hyperparameter Tuning**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=KNeighborsClassifier(),\n",
       "             param_grid={&#x27;n_neighbors&#x27;: [3, 5, 7, 9, 11]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=KNeighborsClassifier(),\n",
       "             param_grid={&#x27;n_neighbors&#x27;: [3, 5, 7, 9, 11]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=KNeighborsClassifier(),\n",
       "             param_grid={'n_neighbors': [3, 5, 7, 9, 11]})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'n_neighbors': [3, 5, 7, 9], 'weights': ['uniform', 'distance']}\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid)\n",
    "grid.fit(X_train, y_train)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Assumptions**:\n",
    "    - KNN assumes that similar things exist in close proximity. In other words, similar things are near to each other.\n",
    "- **Interpretation of Coefficients**: KNN does not provide coefficients as it does not learn a mathematical function from the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <a id='toc1_3_'></a>[Decision Trees](#toc0_)\n",
    "\n",
    "- **Intuition behind test**: Decision Trees is a type of algorithm that makes decisions based on conditions. It's like playing a game of 20 questions to predict the class or value of the target variable.\n",
    "\n",
    "- **Use case for test**: Decision Trees are used for both classification and regression tasks. They are widely used in customer segmentation, detection of fraudulent transactions, or prediction of diseases.\n",
    "\n",
    "- **Intuition for using it for classification**: The tree is constructed in a way that the most important features appear at the top of the tree. It splits the data into subsets based on the feature that provides the most information gain. This process is repeated recursively until it makes a prediction for every subset.\n",
    "\n",
    "- **Intuition for using it for regression**: Decision tree regression observes features of an object and trains a model in the structure of a tree to predict data in the future to produce meaningful continuous output.\n",
    "\n",
    "- **How to code it**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **The most important hyperparameters to iterate through**: `max_depth` (The maximum depth of the tree), `min_samples_split` (The minimum number of samples required to split an internal node), `min_samples_leaf` (The minimum number of samples required to be at a leaf node)\n",
    "\n",
    "- **Code for iterating through one example of a hyperparameter**:\n",
    "\n",
    "```python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'max_depth': [3, 5, 7, 9, 11]}\n",
    "clf = GridSearchCV(DecisionTreeClassifier(), param_grid)\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Assumptions of the algorithm**: Decision tree algorithm assumes that the training data is noise-free, it assumes that missing values are at random, and the most crucial assumption is that the training set is a sample from the actual population."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "- **Intuition**: Linear regression is a statistical model that examines the linear relationship between two (Simple Linear Regression ) or more (Multiple Linear Regression) variables — a dependent variable and independent variable(s). Linear relationship basically means that when one (or more) independent variables increases (or decreases), the dependent variable increases (or decreases) too.\n",
    "- **Use case**: Linear regression is used when we want to predict the value of a variable based on the value of another variable. For example, you could use linear regression to predict sales based on advertising spend.\n",
    "- **Regression Intuition**: Linear regression algorithm finds the best fit line through the data by finding the line that minimizes the sum of squares of residuals.\n",
    "- **Formula**: \n",
    "- **Cost Function**: The cost function in linear regression is the Residual Sum of Squares (RSS) which can be written as:\n",
    "    $$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 $$\n",
    "\n",
    "- **How to code it**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **The most important hyperparameters to iterate through**: `fit_intercept` (Whether to calculate the intercept for this model), `normalize` (This parameter is ignored when fit_intercept is set to False)\n",
    "\n",
    "- **Code for iterating through one example of a hyperparameter**: Linear regression does not typically require hyperparameter tuning.\n",
    "\n",
    "- **Assumptions of the algorithm**: Linear regression assumes that there is a linear relationship between the dependent and independent variables, the residuals are normally distributed and have constant variance, and there is no multicollinearity among independent variables.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines (SVMs)\n",
    "\n",
    "- **Intuition behind test**: SVM is a supervised machine learning algorithm which can be used for both classification or regression challenges. However, it is mostly used in classification problems. In the SVM algorithm, we plot each data item as a point in n-dimensional space (where n is the number of features you have) with the value of each feature being the value of a particular coordinate.\n",
    "\n",
    "- **Use case for test**: SVMs are helpful in text and hypertext categorization, classification of images, and in the biological and other sciences.\n",
    "\n",
    "- **Intuition for using it for classification**: SVMs are based on the idea of finding a hyperplane that best separates the features into different classes.\n",
    "\n",
    "- **Intuition for using it for regression**: In the case of regression, SVMs find the hyperplane that deviates from the most of the data points by no more than a certain amount, and for the rest of the data points, tries to minimize the deviation.\n",
    "\n",
    "- **How to code it**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import svm\n",
    "\n",
    "model = svm.SVC()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **The most important hyperparameters to iterate through**: `C` (Penalty parameter C of the error term), `kernel` (Specifies the kernel type to be used in the algorithm), `gamma` (Kernel coefficient for 'rbf', 'poly' and 'sigmoid')\n",
    "\n",
    "- **Code for iterating through one example of a hyperparameter**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001], 'kernel': ['rbf']} \n",
    "clf = GridSearchCV(svm.SVC(), param_grid)\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Assumptions of the algorithm**: SVMs assume that the data it works with is in a specific format. Namely, that all of the input features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Naive Bayes\n",
    "\n",
    "- **Intuition behind test**: Naive Bayes classifiers are a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with strong (naive) independence assumptions between the features.\n",
    "\n",
    "- **Use case for test**: Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem. They are often used for text classification, spam filtering, and recommendation systems.\n",
    "\n",
    "- **Intuition for using it for classification**: Naive Bayes is a probabilistic classifier, meaning it predicts on the basis of the probability of an object. It uses Bayes' Theorem, which is based on the concept of conditional probability.\n",
    "\n",
    "- **Intuition for using it for regression**: Naive Bayes is not typically used for regression tasks as it's a probabilistic classifier and works based on the assumption of independence among predictors.\n",
    "\n",
    "- **How to code it**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **The most important hyperparameters to iterate through**: Naive Bayes typically doesn't have hyperparameters that need tuning, but some implementations like `BernoulliNB` and `MultinomialNB` have a `alpha` parameter which is a smoothing parameter.\n",
    "\n",
    "- **Code for iterating through one example of a hyperparameter**: Naive Bayes typically doesn't require hyperparameter tuning.\n",
    "\n",
    "- **Assumptions of the algorithm**: Naive Bayes assumes that all features are independent from each other and each one contributes independently to the probability of the outcome. This is a 'naive' assumption because it's rarely true in real-world scenarios."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Linear Regression\n",
    "- **Intuition**: Linear regression is a statistical model that examines the linear relationship between two (Simple Linear Regression ) or more (Multiple Linear Regression) variables — a dependent variable and independent variable(s). Linear relationship basically means that when one (or more) independent variables increases (or decreases), the dependent variable increases (or decreases) too.\n",
    "- **Use case**: Linear regression is used when we want to predict the value of a variable based on the value of another variable. For example, you could use linear regression to predict sales based on advertising spend.\n",
    "- **Regression Intuition**: Linear regression algorithm finds the best fit line through the data by finding the line that minimizes the sum of squares of residuals.\n",
    "- **Probability Formula**: Not applicable for Linear Regression.\n",
    "- **Cost Function**: The cost function in linear regression is the Residual Sum of Squares (RSS) which can be written as:\n",
    "    $$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 $$\n",
    "- **Code Example**:\n",
    "    ```python\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    ```\n",
    "- **Important Hyperparameters**: \n",
    "    - `fit_intercept`: Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations.\n",
    "    - `normalize`: This parameter is ignored when `fit_intercept` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm.\n",
    "- **Code for Hyperparameter Tuning**: Linear Regression does not have hyperparameters that we can tune to improve the performance of the model. The model learns the parameters from the data.\n",
    "- **Assumptions**:\n",
    "    - Linearity: The relationship between X and the mean of Y is linear.\n",
    "    - Homoscedasticity: The variance of residual is the same for any value of X.\n",
    "    - Independence: Observations are independent of each other.\n",
    "    - Normality: For any fixed value of X, Y is normally distributed.\n",
    "- **Interpretation of Coefficients**: The sign of a regression coefficient tells you whether there is a positive or negative correlation between each independent variable the dependent variable. A positive coefficient indicates that as the value of the independent variable increases, the mean of the dependent variable also tends to increase. A negative coefficient suggests that as the independent variable increases, the dependent variable tends to decrease. The coefficient value signifies how much the mean of the dependent variable changes given a one-unit shift in the independent variable while holding other variables in the model constant."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM)\n",
    "- **Intuition**: Support Vector Machines are a set of supervised learning methods used for classification, regression and outliers detection. The idea of SVM is simple: The algorithm creates a line or a hyperplane which separates the data into classes.\n",
    "- **Use case**: SVM is used in a variety of applications such as face detection, intrusion detection, classification of emails, and handwriting recognition.\n",
    "- **Classification Intuition**: SVM can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces. When data are not linearly separable, SVM uses a higher dimension to separate the data, which is not possible using simple logistic regression.\n",
    "- **Regression Intuition**: In addition to performing linear classification, SVMs can efficiently perform a non-linear regression using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.\n",
    "- **Probability Formula**: SVM doesn't directly provide probability estimates. These are calculated using an expensive five-fold cross-validation.\n",
    "- **Cost Function**: SVM aims to minimize the structural risk, which is defined as the sum of the training error and a term that penalizes model complexity (the number of support vectors). The cost function can be written as:\n",
    "    $$ \\min_{w,b,\\xi} \\frac{1}{2}w^Tw + C\\sum_{i=1}^{n}\\xi_i $$\n",
    "    subject to the constraints:\n",
    "    $$ y_i(w^Tx_i - b) \\geq 1 - \\xi_i, \\xi_i \\geq 0 $$\n",
    "- **Code Example**:\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "model = svm.SVC()\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Important Hyperparameters**: \n",
    "    - `C`: Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive.\n",
    "    - `kernel`: Specifies the kernel type to be used in the algorithm. It could be 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or a callable.\n",
    "    - `degree`: Degree of the polynomial kernel function ('poly'). Ignored by all other kernels.\n",
    "    - `gamma`: Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n",
    "- **Code for Hyperparameter Tuning**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001], 'kernel': ['rbf', 'poly', 'sigmoid']}\n",
    "grid = GridSearchCV(svm.SVC(), param_grid, refit=True, verbose=2)\n",
    "grid.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Assumptions**:\n",
    "    - SVM assumes that the data it works with is in a specific format. Namely, a matrix where rows represent the samples and columns represent the attributes of the samples.\n",
    "    - SVM assumes that the data it works with is in a specific format. Namely, a matrix where rows represent the samples and columns represent the attributes of the samples.\n",
    "- **Interpretation of Coefficients**: The coefficients in SVM are not easily interpretable. This is because they do not represent the change in output variable for a unit change in an input variable, as in linear regression. Instead, they represent the weights assigned to the features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
